{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ProtBert-NetOGlyc-classification-training.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOoNaVYppGRqMoFWTd6Cmhi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CasBlaauw/BertOGlyc/blob/main/ProtBert_NetOGlyc_classification_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3Us07TngRPW"
      },
      "source": [
        "Initial model architecture based on [Elnaggar et al. (2020)](https://www.biorxiv.org/content/10.1101/2020.07.12.199554v3.full) and [Heinzinger et al. (2019)](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-3220-8).  \n",
        "Data loader structure inspired by [this pytorch documentation](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html), \n",
        "model architecture inspired by [this pytorch documentation](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html), training loop inspired by [this CNN tutorial](https://chriskhanhtran.github.io/posts/cnn-sentence-classification/), weights for loss function inspired by [this tutorial](https://towardsdatascience.com/handling-class-imbalanced-data-using-a-loss-specifically-made-for-it-6e58fd65ffab), based on [this paper](https://arxiv.org/abs/1901.05555)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vxl162CygIT1"
      },
      "source": [
        "<b>0. Import functions</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5E6UoRvWgIAC"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from datetime import datetime\n",
        "import os\n",
        "from google.colab import files, drive"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJt2CPHWQhzM",
        "outputId": "d785e3de-1d73-466c-adc5-e54c5c401cb2"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxhN3RM5gMxy"
      },
      "source": [
        "<b>1. Read in and split data </b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbnGST746lcM"
      },
      "source": [
        "# Define the Dataset class for use with DataLoader, reading in files as needed\n",
        "\n",
        "class EmbeddingDataset(Dataset):\n",
        "    \"\"\"Dataset of embeddings from ProtBert.\n",
        "    Path is expected to be a path to an zip/npz file containing the .npy arrays for each gene.\n",
        "    Then indexes into those with f\"embed/embeddings_{gene_id}.txt\"\n",
        "    Info is expected to be a pandas dataframe with gene names as keys, 'sequence' and 'label' keys as lists/iterables.\"\"\"\n",
        "    def __init__(self, path, info):\n",
        "        self.path = path\n",
        "        self.info = info.reset_index() \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.info)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        with np.load(self.path) as zip:\n",
        "          embed = zip[f\"embed/embeddings_{self.info['gene'][idx]}.txt\"] # Colab has a weird zip structure + I missed the .txt\n",
        "          embed = embed.T # Need to return transposed because conv1d expects channels, then length\n",
        "        label = torch.tensor(self.info['label'][idx])\n",
        "        return embed, label"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqkwO3P1gMCH"
      },
      "source": [
        "# Define the paths to the info and zip file\n",
        "zip_path = '/content/drive/MyDrive/NetOGlyc/embeddings_npy.zip'\n",
        "info_path = '/content/drive/MyDrive/NetOGlyc/embeddings_info.txt'"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtPkQ4poa9L_",
        "outputId": "77b352ba-f20a-432d-ae74-d9c45ce1339c"
      },
      "source": [
        "# Read in the info file\n",
        "info = pd.read_csv(info_path, sep = '\\t')\n",
        "info['sequence'] = info['sequence'].apply(list)\n",
        "info['label'] = info['label'].apply(lambda x: list(map(int, list(x))))\n",
        "\n",
        "# Split data into test and training files\n",
        "info_train_idx, info_test_idx = train_test_split(range(len(info)), test_size = 0.2)\n",
        "info_train = info.iloc[info_train_idx, :]\n",
        "info_test = info.iloc[info_test_idx, :]\n",
        "print(info_test)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       gene  ...                                              label\n",
            "102  P08572  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "186  P55058  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "338  Q8TBP5  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "201  Q02818  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "227  Q14696  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "..      ...  ...                                                ...\n",
            "244  Q495W5  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "310  Q86VZ4  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "267  Q6L9W6  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "449  Q9UQ53  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "226  Q14667  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "\n",
            "[94 rows x 4 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8flRKdw1Fc2"
      },
      "source": [
        "# Load info and paths into dataset objects and create loades\n",
        "params = {'batch_size': 64, 'shuffle': True}\n",
        "data_train = EmbeddingDataset(zip_path, info_train)\n",
        "loader_train = DataLoader(data_train, **params)\n",
        "data_test = EmbeddingDataset(zip_path, info_test)\n",
        "loader_test = DataLoader(data_test, **params)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3Dpd6ktgTJv"
      },
      "source": [
        "<b>2. Construct the model</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbgX1XkJgXQ1"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels = 1024, out_channels = 32, kernel_size = 7, padding = 3) \n",
        "        self.dropout = nn.Dropout(p=0.25)\n",
        "        self.conv2 = nn.Conv1d(in_channels = 32, out_channels = 2, kernel_size = 7, padding = 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # ---- Layer 1\n",
        "        # conv1 needs (batch_size, in_channels/features, length/seq_len), so (64, 1024, 4000) \n",
        "        # and outputs (64, 32, 4000)\n",
        "        x = self.conv1(x)\n",
        "\n",
        "        # ---- Process first layer's output\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # ---- Layer 2\n",
        "        # conv2 takes (64, 32, 4000) and outputs (64, 2, 4000)\n",
        "        x = self.conv2(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "model = Net()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Agz1MjLfvpG5",
        "outputId": "4e3a8777-9045-4cc4-9f58-d0aa4c4178e1"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "model.to(device)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (conv1): Conv1d(1024, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n",
              "  (dropout): Dropout(p=0.25, inplace=False)\n",
              "  (conv2): Conv1d(32, 2, kernel_size=(7,), stride=(1,), padding=(3,))\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcNVBOMb1hfo"
      },
      "source": [
        "# Loss weights are one of the most important params due to our unbalanced data\n",
        "# Priority here is predicting sites, not predicting non-sites, so we want a very high beta to distinguish between them still.\n",
        "\n",
        "def cb_weights(samples_per_cls, no_of_classes, beta):\n",
        "    \"\"\"Compute the weights for Class Balanced Loss.\n",
        "    Class Balanced Loss: ((1-beta)/(1-beta^n))*Loss(labels, logits)\n",
        "    where Loss is a loss function, here cross entropy loss.\n",
        "    Args:\n",
        "      samples_per_cls: A python list of size [no_of_classes].\n",
        "      no_of_classes: int. total number of classes.\n",
        "      beta: float. Hyperparameter for Class balanced loss.\n",
        "    Returns:\n",
        "      cb_weights: A float tensor of weights to be supplied to the loss function\n",
        "    \"\"\"\n",
        "    effective_num = 1.0 - np.power(beta, samples_per_cls)\n",
        "    weights = (1.0 - beta) / np.array(effective_num)\n",
        "    weights = weights / np.sum(weights) * no_of_classes\n",
        "\n",
        "    return weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QawR-U6zmyAu"
      },
      "source": [
        "total_samples = info['label'].apply(len).sum()\n",
        "total_sites = info['label'].apply(sum).sum() # Assumes 0-1 labeling\n",
        "samples_dist = [total_samples - total_sites, total_sites]\n",
        "weights = cb_weights(samples_dist, len(samples_dist), beta = 0.99999)\n",
        "loss_fn = nn.CrossEntropyLoss(weight = torch.FloatTensor(weights).to(device))\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.001, amsgrad = True)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqfxuEaUNS9j"
      },
      "source": [
        "<b>3. Train the model</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UmeQMvnwixJ"
      },
      "source": [
        "def train(model, optimizer, train_dataloader, val_dataloader=None, epochs=10, checkpoint_path = './'):\n",
        "    \"\"\"Train the CNN model.\n",
        "    Args: \n",
        "      model: the pytorch model object.\n",
        "      optimiser: the loss function, like nn.CrossEntropyLoss(). \n",
        "      train_dataloader: a pytorch DataLoader object that returns the training data, with shape (batch_size, features, length).\n",
        "      val_dataloader: optional. a pytorch DataLoader object like train_dataloader, used to print performance on the test set during training.\n",
        "      epochs: optional. an integer value, indicating the number of epochs (training loops) the training should last.\n",
        "      checkpoint_path: optional. The location to save the model parameter files at every epoch, which are useful to get model values before overfitting sets in.\n",
        "          Note that they currently don't include the optimizer state, so they cannot be used as full checkpoints to continue training later.\n",
        "    Returns:\n",
        "      Doesn't return anything, but has modified the weights of the supplied model object. \n",
        "      Also writes weights per epoch to {checkpoint_path}/model_{date}_{time}_{epoch_i}.pth.\n",
        "      \"\"\"\n",
        "    \n",
        "    # Tracking best validation accuracy\n",
        "    best_accuracy = 0\n",
        "    best_site_accuracy = 0\n",
        "\n",
        "    # Start training loop\n",
        "    print(\"Start training...\\n\")\n",
        "    print(f\"{'Epoch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Val Site Acc':^14} | {'True Pos':^10} | {'True Neg':^10}\")\n",
        "    print(\"-\"*90)\n",
        "\n",
        "    # Save current time \n",
        "\n",
        "    time = datetime.now().strftime(\"%m-%d_%H-%M\")\n",
        "\n",
        "    model = model.float()\n",
        "\n",
        "    for epoch_i in range(epochs):\n",
        "        # =======================================\n",
        "        #               Training\n",
        "        # =======================================\n",
        "\n",
        "        # Tracking time and loss\n",
        "        total_loss = 0\n",
        "\n",
        "        # Put the model into the training mode\n",
        "        model.train()\n",
        "\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            # Load batch to GPU\n",
        "            b_input_ids, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "            # Zero out any previously calculated gradients\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Perform a forward pass. (output shape: (batch, n_classes, length))\n",
        "            logits = model(b_input_ids.float())\n",
        "\n",
        "            # Compute loss and accumulate the loss values\n",
        "            loss = loss_fn(logits, b_labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Perform a backward pass to calculate gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Update parameters\n",
        "            optimizer.step()\n",
        "\n",
        "        # Calculate the average loss over the entire training data\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "        # =======================================\n",
        "        #               Evaluation\n",
        "        # =======================================\n",
        "        if val_dataloader is not None:\n",
        "            # After the completion of each training epoch, measure the model's\n",
        "            # performance on our validation set.\n",
        "            val_loss, val_accuracy, val_site_accuracy, val_tp, val_tn = evaluate(model, val_dataloader)\n",
        "\n",
        "            # Print performance over the entire training data\n",
        "            print(f\"{epoch_i + 1:^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {val_site_accuracy:^14.2f} | {val_tp:^10.2f} | {val_tn:^10.2f}\")\n",
        "        \n",
        "        # =======================================\n",
        "        #               Checkpoint\n",
        "        # =======================================\n",
        "\n",
        "        torch.save(model.state_dict(), os.path.join(checkpoint_path, f\"model_{time}_{epoch_i}.pth\"))\n",
        "\n",
        "    print(f\"Training complete!\")\n",
        "\n",
        "def evaluate(model, val_dataloader):\n",
        "    \"\"\"After the completion of each training epoch, measure the model's\n",
        "    performance on our validation set.\n",
        "    Returns:\n",
        "      val_loss: the mean of the loss across batches.\n",
        "      val_accuracy: the mean of the accuracy (correct predictions) across batches.\n",
        "      val_site_accuracy: the mean of the site accuracy ('1' labels correctly detected) across batches.\n",
        "      val_tp: the mean of the true positive percentage ('1' preditions actually '1' labels) across batches.\n",
        "      val_tn: the mean of the true negative percentage ('0' preditions actually '0' labels) across batches.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled\n",
        "    # during the test time.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    val_accuracy = []\n",
        "    val_site_accuracy = []\n",
        "    val_loss = []\n",
        "    val_tp = []\n",
        "    val_tn = []\n",
        "\n",
        "    # For each batch in our validation set...\n",
        "    for batch in val_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Compute scores (shape: (batch, n_classes, length))\n",
        "        with torch.no_grad():\n",
        "            scores = model(b_input.float())\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_fn(scores, b_labels)\n",
        "        val_loss.append(loss.item())\n",
        "\n",
        "        # Get the predictions\n",
        "        preds = torch.argmax(scores, dim=1)\n",
        "\n",
        "        # Calculate the accuracy rate\n",
        "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
        "        site_accuracy = (preds[b_labels == 1] == b_labels[b_labels == 1]).cpu().numpy().mean()*100\n",
        "        true_pos = (preds[preds == 1] == b_labels[preds == 1]).cpu().numpy().mean()*100\n",
        "        true_neg = (preds[preds == 0] == b_labels[preds == 0]).cpu().numpy().mean()*100\n",
        "        val_accuracy.append(accuracy)\n",
        "        val_site_accuracy.append(site_accuracy)\n",
        "        val_tp.append(true_pos)\n",
        "        val_tn.append(true_neg)\n",
        "\n",
        "    # Compute the average accuracy and loss over the validation set.\n",
        "    val_loss = np.mean(val_loss)\n",
        "    val_accuracy = np.mean(val_accuracy)\n",
        "    val_site_accuracy = np.mean(val_site_accuracy)\n",
        "    val_tp = np.mean(val_tp)\n",
        "    val_tn = np.mean(val_tn)\n",
        "\n",
        "    return val_loss, val_accuracy, val_site_accuracy, val_tp, val_tn"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgJ51Sz_vcRy"
      },
      "source": [
        "!mkdir checkpoint_dir"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjenIAgG3AIR",
        "outputId": "012bc26e-d5eb-4ad2-f91f-abfea13fcefb"
      },
      "source": [
        "train(model, optimizer, loader_train, loader_test, epochs = 20, checkpoint_path = 'checkpoint_dir')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start training...\n",
            "\n",
            " Epoch  |  Train Loss  |  Val Loss  |  Val Acc  |  Val Site Acc  |  True Pos  |  True Neg \n",
            "------------------------------------------------------------------------------------------\n",
            "   1    |   0.631109   |  0.590762  |   99.82   |     10.15      |    9.63    |   99.91   \n",
            "   2    |   0.574027   |  0.562861  |   98.58   |     75.95      |    5.60    |   99.98   \n",
            "   3    |   0.543790   |  0.541366  |   99.30   |     68.79      |    9.67    |   99.97   \n",
            "   4    |   0.513176   |  0.497464  |   99.11   |     74.56      |    7.66    |   99.98   \n",
            "   5    |   0.481214   |  0.464758  |   99.11   |     76.13      |    8.14    |   99.98   \n",
            "   6    |   0.446107   |  0.422800  |   99.14   |     82.48      |    9.23    |   99.98   \n",
            "   7    |   0.406868   |  0.382631  |   99.19   |     81.82      |    9.55    |   99.98   \n",
            "   8    |   0.361281   |  0.337564  |   99.18   |     82.93      |    8.99    |   99.98   \n",
            "   9    |   0.312608   |  0.289396  |   99.17   |     84.12      |    9.44    |   99.98   \n",
            "  10    |   0.263413   |  0.246042  |   99.05   |     83.63      |    9.01    |   99.98   \n",
            "  11    |   0.218432   |  0.208965  |   99.33   |     81.22      |   11.43    |   99.98   \n",
            "  12    |   0.176521   |  0.165791  |   99.20   |     86.23      |    9.57    |   99.99   \n",
            "  13    |   0.143374   |  0.141976  |   99.28   |     78.94      |   10.65    |   99.98   \n",
            "  14    |   0.115853   |  0.118210  |   99.17   |     84.22      |    9.38    |   99.98   \n",
            "  15    |   0.095100   |  0.103479  |   99.29   |     81.45      |   11.28    |   99.98   \n",
            "  16    |   0.080048   |  0.094119  |   99.09   |     83.38      |    9.17    |   99.98   \n",
            "  17    |   0.067393   |  0.088102  |   99.23   |     80.83      |   10.28    |   99.98   \n",
            "  18    |   0.058505   |  0.079690  |   99.39   |     77.78      |   11.63    |   99.98   \n",
            "  19    |   0.052315   |  0.072386  |   99.29   |     80.96      |   10.07    |   99.98   \n",
            "  20    |   0.046699   |  0.064914  |   99.19   |     85.62      |    9.16    |   99.99   \n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YojIJ3DUMlur"
      },
      "source": [
        "<b>4. Save and export the model</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoR_FZVFEocN"
      },
      "source": [
        "torch.save(model.state_dict(), \"model_params.pth\")\n",
        "torch.save(model, \"model_full.pth\")"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctWjoBW57Q-i",
        "outputId": "7746175b-2871-43f4-ca12-fd2f53d1c925"
      },
      "source": [
        "!zip  model_checkpoints.zip checkpoint_dir/model_06-14*.*"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: checkpoint_dir/model_06-14_12-05_0.pth (deflated 7%)\n",
            "  adding: checkpoint_dir/model_06-14_12-05_10.pth (deflated 8%)\n",
            "  adding: checkpoint_dir/model_06-14_12-05_11.pth (deflated 7%)\n",
            "  adding: checkpoint_dir/model_06-14_12-05_12.pth (deflated 7%)\n",
            "  adding: checkpoint_dir/model_06-14_12-05_13.pth (deflated 7%)\n",
            "  adding: checkpoint_dir/model_06-14_12-05_14.pth (deflated 7%)\n",
            "  adding: checkpoint_dir/model_06-14_12-05_15.pth (deflated 7%)\n",
            "  adding: checkpoint_dir/model_06-14_12-05_16.pth (deflated 7%)\n",
            "  adding: checkpoint_dir/model_06-14_12-05_17.pth (deflated 7%)\n",
            "  adding: checkpoint_dir/model_06-14_12-05_18.pth (deflated 7%)\n",
            "  adding: checkpoint_dir/model_06-14_12-05_19.pth (deflated 7%)\n",
            "  adding: checkpoint_dir/model_06-14_12-05_1.pth (deflated 7%)\n",
            "  adding: checkpoint_dir/model_06-14_12-05_2.pth (deflated 8%)\n",
            "  adding: checkpoint_dir/model_06-14_12-05_3.pth (deflated 8%)\n",
            "  adding: checkpoint_dir/model_06-14_12-05_4.pth (deflated 8%)\n",
            "  adding: checkpoint_dir/model_06-14_12-05_5.pth (deflated 8%)\n",
            "  adding: checkpoint_dir/model_06-14_12-05_6.pth (deflated 8%)\n",
            "  adding: checkpoint_dir/model_06-14_12-05_7.pth (deflated 8%)\n",
            "  adding: checkpoint_dir/model_06-14_12-05_8.pth (deflated 8%)\n",
            "  adding: checkpoint_dir/model_06-14_12-05_9.pth (deflated 8%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qvl7j9prEpi3"
      },
      "source": [
        "!cp model_params.pth /content/drive/MyDrive/NetOGlyc/\n",
        "!cp model_full.pth /content/drive/MyDrive/NetOGlyc/\n",
        "!cp model_checkpoints.zip /content/drive/MyDrive/NetOGlyc/"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyuaNZBsiQiV"
      },
      "source": [
        "drive.flush_and_unmount()"
      ],
      "execution_count": 28,
      "outputs": []
    }
  ]
}