{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "name": "ProtBert-BFD-NetOGlyc.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CasBlaauw/BertOGlyc/blob/main/ProtBert_BFD_NetOGlyc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXFGYhQo-h2F"
      },
      "source": [
        "<h3> Extracting protein sequences' features using ProtBert-BFD pretrained-model <h3>\n",
        "\n",
        "Builds on [the code](https://github.com/agemagician/ProtTrans/blob/master/Embedding/PyTorch/Basic/ProtBert-BFD.ipynb) from [Alnaggar et al. (2020)](https://www.biorxiv.org/content/10.1101/2020.07.12.199554v3.full)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRKqEJ5L-h2H"
      },
      "source": [
        "<b>1. Load necessary libraries including huggingface transformers<b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXAKFATm-mbs"
      },
      "source": [
        "!pip install -q transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jd3YQUd1-h2I"
      },
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel, pipeline\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "from google.colab import files, drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAKCMu_2-h2V"
      },
      "source": [
        "<b>2. Load the vocabulary and ProtBert-BFD Model<b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HS8i5sOJ-h2W"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"Rostlab/prot_bert_bfd\", do_lower_case=False )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERtkR05t-h2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fea92a1-a26a-48e5-8ad4-b0e7d133ee49"
      },
      "source": [
        "model = AutoModel.from_pretrained(\"Rostlab/prot_bert_bfd\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDnhEiyI-h2g"
      },
      "source": [
        "<b>3. Load the model into the GPU if avilabile<b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNweYMPQ-h2h"
      },
      "source": [
        "fe = pipeline('feature-extraction', model=model, tokenizer=tokenizer, device=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWbZG3wu-h2l"
      },
      "source": [
        "<b>4. Preprocess data<b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kqLvaxa-h2l"
      },
      "source": [
        "# sequences_Example = [\"A E T C Z A O\",\"S K T Z P\"]\n",
        "# sequences_Example = [re.sub(r\"[UZOB]\", \"X\", sequence) for sequence in sequences_Example]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxqxgOXDZEos"
      },
      "source": [
        "# Read in the sequences column\n",
        "sequences = pd.read_csv('glycosites_unique_filtered.tsv', sep = '\\t', usecols = ['sequence'], squeeze = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbdLlFitZHB6"
      },
      "source": [
        "# Map rarely used amino acids to X (don't think these exist in our data)\n",
        "sequences = sequences.str.replace(r\"[UZOB]\", \"X\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdvu_9PhGY-W"
      },
      "source": [
        "# Set maximum length, all genes shorter are filtered\n",
        "max_len = 4000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMl8FS5aJ7_j"
      },
      "source": [
        "# Read in the info\n",
        "glycosites = pd.read_csv('glycosites_unique_filtered.tsv', sep = '\\t')\n",
        "# Filter out long genes\n",
        "glycosites = glycosites[glycosites['sequence'].str.len() < max_len]\n",
        "glycosites.reset_index(inplace=True, drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLxGKOwe-h2r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28fcf96d-185e-4eef-9f83-886e9120efcc"
      },
      "source": [
        "# Drop ridiculously huge genes because they crash the model...\n",
        "# Up to 10000: already filtered out in glycosites_unique_filtered.tsv\n",
        "# Model can handle up to 10000, but decided to train CNN on up to 4000 for now because padding is added up to max\n",
        "removed = np.where(sequences.str.len() >= max_len)[0].tolist()\n",
        "print(f'Dropping sequences #{removed}')\n",
        "print(len(sequences))\n",
        "sequences = sequences[sequences.str.len() < max_len]\n",
        "sequences.reset_index(inplace=True, drop=True)\n",
        "print(sequences)\n",
        "\n",
        "# Store the lengths\n",
        "seq_lens = sequences.str.len().copy()\n",
        "print('Top lengths: ', sorted(seq_lens, reverse=True)[:20])\n",
        "\n",
        "# Tokenize sequences by interlacing spaces\n",
        "sequences = sequences.map(' '.join)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dropping sequences #[85, 200, 201, 210, 230, 265, 318, 415, 431]\n",
            "475\n",
            "0      MAIDRRREAAGGGPGRQPAPAEENGSLPPGDAAASAPLGGRAGPGG...\n",
            "1      MRVLACLLAALVGIQAVERLRLADGPHGCAGRLEVWHGGRWGTVCD...\n",
            "2      MNKTNQVYAANEDHNSQFIDDYSSSDESLSVSHFSFSKQSHRPRTI...\n",
            "3      MGVAARPPALRHWFSHSIPLAIFALLLLYLSVRSLGARSGCGPRAQ...\n",
            "4      MARHGCLGLGLFCCVLFAATVGPQPTPSIPGAPATTLTPVPQSEAS...\n",
            "                             ...                        \n",
            "461    MTPQSLLQTTLFLLSLLFLVQGAHGRGHREDFRFCSQRNQTHRSSL...\n",
            "462    MGQRLSGGRSCLDVPGRLLPQPPPPPPPVRRKLALLFAMLCVWLYM...\n",
            "463    MAPRTLWSCYLCCLLTAAAGAASYPPRGFSLYTGSSGALSPGGPQA...\n",
            "464    MPRATALGALVSLLLLLPLPRGAGGLGERPDATADYSELDGEEGTE...\n",
            "465    MKWKHVPFLVMISLLSLSPNHLFLAQLIPDPEDVERGNDHGTPIPT...\n",
            "Name: sequence, Length: 466, dtype: object\n",
            "Top lengths:  [3396, 3333, 3230, 3063, 3014, 2912, 2828, 2623, 2595, 2413, 2386, 2346, 2315, 2235, 2224, 2214, 2179, 2169, 2135, 2045]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKSQcOA90Mqt"
      },
      "source": [
        "!mkdir embed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMjmypcI7RzN"
      },
      "source": [
        "# Option 1: Output for CNN\n",
        "- Adds padding to embeddings and sequences up to `max_len`\n",
        "- Outputs embeddings as a zip of .npy arrays for each gene, of (`max_len`,1024) each\n",
        "- Outputs info as a single tab-separated .txt (`n_seqs`, 3), with genes as rows (set as index labels) and `['sequence', 'sites', 'label']` as columns. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfk9vkyV-h2x"
      },
      "source": [
        "<b>5. Extract sequences' features, add padding, and write to file<b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTi2gR06GIwd"
      },
      "source": [
        "# Optionally turn off making embedding/info if not needed\n",
        "write_embedding = False\n",
        "write_info = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLlwS0GqaRoP"
      },
      "source": [
        "index = 0\n",
        "n_seqs = len(glycosites)\n",
        "\n",
        "# Add scaffold for labels\n",
        "if write_info:\n",
        "  glycosites = pd.concat([glycosites, pd.Series(['0'*max_len]*len(glycosites), name = 'label')], axis = 1)\n",
        "\n",
        "# Start loop\n",
        "for seq in sequences:\n",
        "  # ----- Keeping track -----\n",
        "  print(f'{index+1} / {n_seqs}')\n",
        "  pad_len = max_len - seq_lens[index]\n",
        "  msg = 'Length before padding: '\n",
        "  msg_pad = 'Length after padding: '\n",
        "\n",
        "  # ----- Embedding -----\n",
        "  if write_embedding:\n",
        "    # Get the embedding for each sequence\n",
        "    embedding = fe([seq])\n",
        "\n",
        "    # Remove any special tokens ([PAD],[CLS],[SEP]) added by model\n",
        "    embedding = np.array(embedding)[0, 1:(seq_lens[index]+1), :]\n",
        "    msg += 'embedding: ' + str(embedding.shape)    \n",
        "\n",
        "    # Add padding\n",
        "    embedding_padding = np.zeros((pad_len, 1024))\n",
        "    embedding = np.append(embedding, embedding_padding, axis = 0)\n",
        "\n",
        "    msg_pad += 'embedding: ' + str(embedding.shape)\n",
        "\n",
        "    # Write embedding to file\n",
        "    np.save(f\"embed/embeddings_{gene[0]}\", embedding)\n",
        "\n",
        "  # ----- Info -----\n",
        "  if write_info:\n",
        "    msg += 'protein seq: ' + str(len(glycosites.loc[index, 'sequence']))\n",
        "\n",
        "    # Add padding to sequence\n",
        "    glycosites.loc[index, 'sequence'] += '-'*pad_len\n",
        "    msg_pad += 'protein seq: ' + str(len(glycosites.loc[index, 'sequence']))\n",
        "\n",
        "    # Set labels to positive at sites\n",
        "    sites = glycosites['sites'][index].split(' ')\n",
        "    site_ids = [int(site[1:])-1 for site in sites]\n",
        "    label = ''.join(['1' if idx in site_ids else '0' for idx in range(max_len)])\n",
        "    glycosites.loc[index, 'label'] = label\n",
        "    print('Site w/ res in seq:', [(site, glycosites.loc[index, 'sequence'][site_id]) for site, site_id in zip(sites, site_ids)]) # Sanity check that site matches seq at id\n",
        "\n",
        "  # ----- Housekeeping -----\n",
        "  print(msg, msg_pad)\n",
        "  index += 1\n",
        "  gc.collect()\n",
        "\n",
        "# Write info to file\n",
        "if write_info:\n",
        "  glycosites.set_index('gene', inplace = True)\n",
        "  print(glycosites.head())\n",
        "  glycosites.to_csv('embeddings_info.txt', sep = '\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2PJFS0XFTPu"
      },
      "source": [
        "<b>6. Export files</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCijLjIf0m-X"
      },
      "source": [
        "!zip -r /content/embeddings_npy.zip embed/embeddings_*.*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9hCuF7Gr5AA"
      },
      "source": [
        "drive.mount('/content/drive')\n",
        "if write_embedding:\n",
        "  !cp /content/embeddings_npy.zip /content/drive/MyDrive/NetOGlyc\n",
        "if write_info:\n",
        "  !cp /content/embeddings_info.txt /content/drive/MyDrive/NetOGlyc\n",
        "drive.flush_and_unmount()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzxWGHq69L-0"
      },
      "source": [
        "# Option 2: Just embeddings, as .txt in zip\n",
        "- Doesn't pad sequences\n",
        "- Outputs embeddings as a zip of .txt files for each gene, of (prot_len, 1024) each\n",
        "- Doesn't output info - can use glycosites file, but make sure `max_len` filtering is the same! (`glycosites_unique_filtered.txt` is filtered to 10000, current `max_len` is 4000)\n",
        "\n",
        "**Semi-deprecated, as the rest of the workflow assumes .npy arrays in zip files.** Might need some changes in 1-4 to work again (thinking mostly about the label column, but maybe other things too), as I changed those to after switching to option 1. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZ3t3F1wFIVV"
      },
      "source": [
        "<b>5. Extract sequences' features and remove padding/special tokens<b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZPpLMK2-h2y"
      },
      "source": [
        "index = 0\n",
        "n_seqs = len(sequences)\n",
        "for seq in sequences:\n",
        "  # Keeping track\n",
        "  print(f'{index+1} / {n_seqs}')\n",
        "  # Get the embedding for each sequence\n",
        "  embedding = fe([seq])\n",
        "  # Remove padding ([PAD]) and special tokens ([CLS],[SEP]) added by model\n",
        "  embedding = np.array(embedding)[0, 1:(seq_lens[index]+1), :]\n",
        "  print(f\"Embedding size: {embedding.shape}, seq_len {seq_lens[index]}\")\n",
        "  # Save embeddings to file, matrix of (prot_len x 1024) per protein\n",
        "  np.savetxt(f\"embed/embeddings_{glycosites['gene'][index]}.txt\", embedding, delimiter = '\\t')\n",
        "  # Housekeeping to prepare for next loop\n",
        "  index += 1\n",
        "  gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkXk78UC1MGT"
      },
      "source": [
        "<b>6. Export files</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGf3NeTitz45"
      },
      "source": [
        "!zip -r /content/embeddings_individual_filtered.zip /content/embed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfFU196i1HkC",
        "outputId": "22eaa9ea-42ae-4bc4-edb9-fa4285f701eb"
      },
      "source": [
        "# Download zip: very slow, faster to move to drive and download/access from there\n",
        "# files.download(\"/content/embeddings_individual.zip\")\n",
        "drive.mount('/content/drive')\n",
        "!cp /content/embeddings_individual_filtered.zip /content/drive/MyDrive/NetOGlyc/\n",
        "drive.flush_and_unmount()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhP6ph0U-SyV"
      },
      "source": [
        "# Option 3: Embeddings and data in one big file\n",
        "- Doesn't pad sequences\n",
        "- Outputs embeddings as a zip of one big .txt file, with all genes concatenated and a residue on each row (`seq_lens.sum()`, 1027) \n",
        "- Info is included in the main file, as first three columns: `['gene', 'residue', 'label']`, after which `['0', '1', ... , '1023']` start.\n",
        "- Note that this .txt file is therefore made to be imported by pandas, numpy won't like those label columns. `data.loc[:, '0':'1023'].to_numpy()` should work, at the cost of loading everything into memory twice. Also turns out to be pretty useless if you're trying to train a CNN on genes, not individual embeddings.\n",
        "\n",
        "**Semi-deprecated, as the rest of the workflow assumes .npy arrays in zip files.** Hasn't been touched for a bit, so might not have all the improvements option 1 has."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bE55hskgZdX1"
      },
      "source": [
        "<b> 5-6. Alternative: write data with aa and gene name to one big file </b>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEhBiZL7mfU1"
      },
      "source": [
        "with open('embed/embeddings.txt', 'w') as file:\n",
        "  index = 0\n",
        "  header = True\n",
        "  n_seqs = len(glycosites)\n",
        "  for seq in sequences:\n",
        "    # Keeping track\n",
        "    print(f'{index+1} / {n_seqs}')\n",
        "\n",
        "    # Get the embedding for each sequence\n",
        "    embedding = fe([seq])\n",
        "\n",
        "    # Remove padding ([PAD]) and special tokens ([CLS],[SEP]) added by model\n",
        "    embedding = np.array(embedding)[0, 1:(seq_lens[index]+1), :]\n",
        "\n",
        "    # Prepare gene/residue/site labels\n",
        "    gene = pd.Series([glycosites.iloc[index, 0]]*seq_lens[index], name = 'gene')\n",
        "    prot_seq = pd.Series(list(seq.replace(\" \", \"\")), name = 'residue')\n",
        "    sites = pd.Series(glycosites.iloc[index, 1].split(' '), name = 'sites')\n",
        "    label = pd.Series([0]*seq_lens[index], name = 'label')\n",
        "\n",
        "    # Bind into one dataframe\n",
        "    embedding = pd.DataFrame(embedding)\n",
        "    print(f'shapes: gene {gene.shape}, prot {prot_seq.shape}, embedding {embedding.shape}')\n",
        "    embedding = pd.concat([gene, prot_seq, label, embedding], axis = 1)\n",
        "\n",
        "    # Add sites\n",
        "    for site in sites:\n",
        "      site_index = int(site[1:])-1\n",
        "      embedding.iloc[site_index, 2] = 1\n",
        "      # print(f\"gene: {embedding['gene'][site_index]}, site: {site}, prot residue: {embedding['residue'][site_index]}\")\n",
        "    print([(site, embedding['residue'][int(site[1:])-1]) for site in sites])\n",
        "\n",
        "    # Write to file\n",
        "    embedding.to_csv(file, sep = '\\t', header = header, index = False, mode = 'a')\n",
        "\n",
        "    # Housekeeping\n",
        "    index += 1\n",
        "    header = False\n",
        "    gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlHYJe3xB1Na"
      },
      "source": [
        "!zip /content/embeddings_txt.zip /content/embeddings.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVbVX85SC7cb"
      },
      "source": [
        "drive.mount('/content/drive')\n",
        "!cp /content/embeddings_txt.zip /content/drive/MyDrive/NetOGlyc/\n",
        "drive.flush_and_unmount()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}