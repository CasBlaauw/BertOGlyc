{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BertOGlyc-train-NN-Lauge.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/casblaauw/BertOGlyc/blob/main/BertOGlyc_train_per_protein_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3Us07TngRPW"
      },
      "source": [
        "Initial model architecture based on [Elnaggar et al. (2020)](https://www.biorxiv.org/content/10.1101/2020.07.12.199554v3.full) and [Heinzinger et al. (2019)](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-3220-8).  \n",
        "Data loader structure inspired by [this pytorch documentation](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html), \n",
        "model architecture inspired by [this pytorch documentation](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html), training loop inspired by [this CNN tutorial](https://chriskhanhtran.github.io/posts/cnn-sentence-classification/), weights for loss function inspired by [this tutorial](https://towardsdatascience.com/handling-class-imbalanced-data-using-a-loss-specifically-made-for-it-6e58fd65ffab), based on [this paper](https://arxiv.org/abs/1901.05555). Hyperparameter tuning based on [this tutorial](https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html) and general [Ray docs](https://docs.ray.io/en/latest/tune/key-concepts.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vxl162CygIT1"
      },
      "source": [
        "<b>0. Import functions</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILN5pWgQsKts",
        "outputId": "17f68c14-ea2b-4724-eaf7-d78c87b7069e"
      },
      "source": [
        "!pip install ray\n",
        "!pip install -U hyperopt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ray\n",
            "  Downloading ray-1.9.0-cp37-cp37m-manylinux2014_x86_64.whl (57.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 57.6 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray) (2.6.0)\n",
            "Requirement already satisfied: grpcio>=1.28.1 in /usr/local/lib/python3.7/dist-packages (from ray) (1.42.0)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from ray) (21.2.0)\n",
            "Collecting redis>=3.5.0\n",
            "  Downloading redis-4.0.2-py3-none-any.whl (119 kB)\n",
            "\u001b[K     |████████████████████████████████| 119 kB 74.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from ray) (7.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from ray) (3.13)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from ray) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.15.3 in /usr/local/lib/python3.7/dist-packages (from ray) (3.17.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray) (3.4.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray) (1.0.3)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio>=1.28.1->ray) (1.15.0)\n",
            "Collecting deprecated\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated->redis>=3.5.0->ray) (1.13.3)\n",
            "Installing collected packages: deprecated, redis, ray\n",
            "Successfully installed deprecated-1.2.13 ray-1.9.0 redis-4.0.2\n",
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.7/dist-packages (0.1.2)\n",
            "Collecting hyperopt\n",
            "  Downloading hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 15.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from hyperopt) (1.15.0)\n",
            "Collecting py4j\n",
            "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 71.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from hyperopt) (1.19.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from hyperopt) (0.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from hyperopt) (4.62.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from hyperopt) (1.4.1)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.7/dist-packages (from hyperopt) (2.6.3)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from hyperopt) (1.3.0)\n",
            "Installing collected packages: py4j, hyperopt\n",
            "  Attempting uninstall: hyperopt\n",
            "    Found existing installation: hyperopt 0.1.2\n",
            "    Uninstalling hyperopt-0.1.2:\n",
            "      Successfully uninstalled hyperopt-0.1.2\n",
            "Successfully installed hyperopt-0.2.7 py4j-0.10.9.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5E6UoRvWgIAC"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from ray import tune\n",
        "from ray.tune import JupyterNotebookReporter\n",
        "from ray.tune.schedulers import ASHAScheduler\n",
        "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
        "from functools import partial\n",
        "\n",
        "import os\n",
        "from google.colab import files, drive\n",
        "import gc"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJt2CPHWQhzM",
        "outputId": "37e29450-cf01-46db-a50c-9897206a3437"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxhN3RM5gMxy"
      },
      "source": [
        "<b>1. Read in and pre-split data </b>\n",
        "\n",
        "The info data frame determines the contents of the dataset. Indexing the dataset (as done by the data loader) looks at the supplied info file, gets the gene/protein ID associated with that position, and retrieves that protein's embeddings from the zip file.   \n",
        "\n",
        "Therefore, the data can be split by simply splitting the info data frame and building EmbeddingDatasets/DataLoaders with those. The actual construction of those happens within the training function (to split into random train/validation sets for each training run) and at post-training testing, but the info dataframes are already prepared here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbnGST746lcM"
      },
      "source": [
        "# Define the Dataset class for use with DataLoader, reading in files as needed\n",
        "class EmbeddingDataset(Dataset):\n",
        "    \"\"\"Dataset of embeddings from ProtBert.\n",
        "    Path is expected to be a path to an zip/npz file containing the .npy arrays for each gene.\n",
        "    Then indexes into that zip file to find the f\"embeddings_{gene_id} file.\"\n",
        "    Info is expected to be a pandas dataframe containing a uniprot column corresponding to zip IDs and a label column with numeric labels.\"\"\"\n",
        "    def __init__(self, path, info):\n",
        "        self.path = path\n",
        "        self.info = info.reset_index(drop=True) \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.info)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        with np.load(self.path) as zip:\n",
        "          embed = zip[self.info['uniprot'][idx]] \n",
        "          embed = embed.mean(axis = 0, keepdims = False) # Mean-pool embeddings into single 1024-length embedding for protein\n",
        "        label = torch.tensor(self.info['label'][idx])\n",
        "        return embed, label"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqkwO3P1gMCH"
      },
      "source": [
        "# Define the paths to the info and zip file\n",
        "zip_path = '/content/drive/MyDrive/data/[sequence_embeddings].npz'\n",
        "info_path = '/content/drive/MyDrive/data/[site_info].csv'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtPkQ4poa9L_",
        "outputId": "fb3208a0-cde3-40c6-a2d2-0835d2cfb393"
      },
      "source": [
        "# Read in the info file\n",
        "info = pd.read_csv(info_path, index_col = 0)\n",
        "info = info.loc[:, ('uniprot', 'localisation')].drop_duplicates().dropna().reset_index(drop=True) # Need one protein per row, so keep only protein-wide info and drop duplicates\n",
        "info['label'] = info['localisation'].map({'cytosolic': 0, 'extracellular': 1})\n",
        "\n",
        "# Split data into test and training files\n",
        "trainval_idx, test_idx = train_test_split(range(len(info)), test_size = 0.2)\n",
        "\n",
        "info_trainval = info.iloc[trainval_idx, :]\n",
        "info_test = info.iloc[test_idx, :]\n",
        "print(info_test)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    uniprot   localisation  label\n",
            "303  P38797  extracellular      1\n",
            "469  Q04947  extracellular      1\n",
            "734  O94399      cytosolic      0\n",
            "83   P23644      cytosolic      0\n",
            "358  P32319  extracellular      1\n",
            "..      ...            ...    ...\n",
            "593  P53304  extracellular      1\n",
            "350  Q12025  extracellular      1\n",
            "779  Q9UR09  extracellular      1\n",
            "576  P43497  extracellular      1\n",
            "663  P40092  extracellular      1\n",
            "\n",
            "[162 rows x 3 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (5,6,7,14,15,18) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3Dpd6ktgTJv"
      },
      "source": [
        "<b>2. Define model and function for weights</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbgX1XkJgXQ1"
      },
      "source": [
        "# class Net(nn.Module):\n",
        "#     def __init__(self, kernel_size = 7):\n",
        "#         super(Net, self).__init__()\n",
        "#         self.conv1 = nn.Conv1d(in_channels = 1024, out_channels = 32, kernel_size = kernel_size, padding = kernel_size//2) \n",
        "#         self.dropout = nn.Dropout(p=0.25)\n",
        "#         self.conv2 = nn.Conv1d(in_channels = 32, out_channels = 2, kernel_size = kernel_size, padding = kernel_size//2)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # ---- Layer 1\n",
        "#         # conv1 needs (batch_size, in_channels/features, length/seq_len), so (64, 1024, 4000) \n",
        "#         # and outputs (64, 32, 4000)\n",
        "#         x = self.conv1(x)\n",
        "\n",
        "#         # ---- Process first layer's output\n",
        "#         x = self.dropout(x)\n",
        "#         x = F.relu(x)\n",
        "\n",
        "#         # ---- Layer 2\n",
        "#         # conv2 takes (64, 32, 4000) and outputs (64, 2, 4000)\n",
        "#         x = self.conv2(x)\n",
        "        \n",
        "#         return x\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.lin1 = nn.Linear(in_features = 1024, out_features = 128) \n",
        "        self.lin2 = nn.Linear(in_features = 128, out_features = 10)\n",
        "        self.lin3 = nn.Linear(in_features = 10, out_features = 2)\n",
        "        self.dropout = nn.Dropout(p=0.25)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # ---- Layer 1\n",
        "        # lin1 needs (batch_size, in_features), so (64, 1024) \n",
        "        # and outputs (64, 128)\n",
        "        x = self.lin1(x)\n",
        "\n",
        "        # ---- Process first layer's output\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # ---- Layer 2\n",
        "        # lin2 takes (64, 128) and outputs (64, 10)\n",
        "        x = self.lin2(x)\n",
        "\n",
        "        # ---- Process second layer's output\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # ---- Layer 3\n",
        "        # lin3 takes (64, 10) and outputs (64, 2)\n",
        "        x = self.lin3(x)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcNVBOMb1hfo"
      },
      "source": [
        "# Loss weights can help prioritise properly predicting glycosites over the bulk unglycosylated sites\n",
        "    \n",
        "def weighted_weights(labels, balance_factor = 1):\n",
        "  \"\"\"Compute the weights for the loss function, weighted by proportion in the data.\n",
        "  Args:\n",
        "    labels: Pandas series of labels. Assumes binary 0/1 labeling.\n",
        "    balance_factor: float. Hyperparameter for loss balancing. Default is 1.\n",
        "      Increasing it above 1 will make 1-labels more important than their proportion in the data, and vice versa.\n",
        "      If set to None, will not balance weights and return equal [1,1] weights.\n",
        "  Returns:\n",
        "    A set of weights to be supplied to the loss function.\n",
        "  \"\"\"\n",
        "  # total_samples = labels.apply(len).sum()\n",
        "  # total_sites = labels.apply(sum).sum() # Assumes 0-1 labeling\n",
        "  # total_nonsites = total_samples - total_sites\n",
        "\n",
        "  if balance_factor is None:\n",
        "    return [1, 1]\n",
        "\n",
        "  total_sites = labels.sum()\n",
        "  total_nonsites = labels.len() - total_sites\n",
        "  weights = [1, balance_factor * (total_nonsites / total_sites)]\n",
        "  return weights"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anF-WMUyJ5Lj"
      },
      "source": [
        "**3. Define training and tuning functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UmeQMvnwixJ"
      },
      "source": [
        "def train(config, device, zip_path, info, epochs=10, tuning = False, checkpoint_dir = None):\n",
        "    \"\"\"Train the NN model.\n",
        "    Args: \n",
        "      config: a dictionary with hyperparameter values {'loss_balance_factor', 'lr'}. \n",
        "        If tuning = True, supports ray.tune search spaces.\n",
        "      device: a pytorch device indicating whether the model should be loaded into cpu or gpu.\n",
        "      zip_path: a path to an zip/npz file containing the .npy arrays for each gene.\n",
        "      info: a pandas dataframe with gene names as keys, 'sequence' and 'label' keys as lists/iterables.\n",
        "      epochs: optional. an integer value, indicating the number of epochs (training loops) the training should last.\n",
        "      tuning: optional. a boolean indicating whether the model is ran in the context of ray.tune tuning.\n",
        "        In that case, it won't print training results, but will instead pass them to ray.tune.\n",
        "        Default = False.\n",
        "      checkpoint_dir: optional. only used when tuning = true. used to retrieve the best model's model_state after tuning.\n",
        "        Default = None.\n",
        "    Returns:\n",
        "      Doesn't return anything, but has modified the weights of the supplied model object. \n",
        "      \"\"\"\n",
        "\n",
        "    # Initialise model\n",
        "    model = Net()\n",
        "    model.to(device)\n",
        "\n",
        "    # Load info and paths into dataset objects and create loaders\n",
        "    train_idx, val_idx = train_test_split(range(len(info)), test_size = 0.2)\n",
        "    loader_params = {'batch_size': 64, 'shuffle': True}\n",
        "\n",
        "    info_train = info.iloc[train_idx, :]\n",
        "    data_train = EmbeddingDataset(zip_path, info_train)\n",
        "    loader_train = DataLoader(data_train, **loader_params)\n",
        "\n",
        "    info_val = info.iloc[val_idx, :]\n",
        "    data_val = EmbeddingDataset(zip_path, info_val)\n",
        "    loader_val = DataLoader(data_val, **loader_params)\n",
        "\n",
        "    # Gather weights\n",
        "    # weights = weighted_weights(info_train['label'], balance_factor = config['loss_balance_factor'])\n",
        "    weights = [1,1]\n",
        "\n",
        "    # Initialise loss function and optimizer \n",
        "    loss_fn = nn.CrossEntropyLoss(weight = torch.FloatTensor(weights).to(device))\n",
        "    optimizer = optim.Adam(model.parameters(), lr = config['lr'], amsgrad = True) \n",
        "\n",
        "    if tuning and checkpoint_dir:\n",
        "        model_state, optimizer_state = torch.load(\n",
        "            os.path.join(checkpoint_dir, \"checkpoint\"))\n",
        "        model.load_state_dict(model_state)\n",
        "        optimizer.load_state_dict(optimizer_state)\n",
        "\n",
        "    # Start training loop\n",
        "    if not tuning:\n",
        "      print(\"Start training...\\n\")\n",
        "      print(f\"{'Epoch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Prec. @.5':^10} | {'Recall@.5':^10} | {'AUC':^10} | {'AP':^10}\")\n",
        "      print(\"-\"*87)\n",
        "\n",
        "    model = model.float()\n",
        "\n",
        "    for epoch_i in range(epochs):\n",
        "        # =======================================\n",
        "        #               Training\n",
        "        # =======================================\n",
        "\n",
        "        # Track loss\n",
        "        total_loss = 0\n",
        "\n",
        "        # Put the model into the training mode\n",
        "        model.train()\n",
        "\n",
        "        for step, batch in enumerate(loader_train):\n",
        "            # Load batch to GPU\n",
        "            b_input_ids, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "            # Zero out any previously calculated gradients\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Perform a forward pass. (output shape: (batch, n_classes, length))\n",
        "            logits = model(b_input_ids.float())\n",
        "\n",
        "            # Compute loss and accumulate the loss values\n",
        "            loss = loss_fn(logits, b_labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Perform a backward pass to calculate gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Update parameters\n",
        "            optimizer.step()\n",
        "\n",
        "        # Calculate the average loss over the entire training data\n",
        "        avg_train_loss = total_loss / len(loader_train)\n",
        "\n",
        "        # =======================================\n",
        "        #               Evaluation\n",
        "        # =======================================\n",
        "        # After the completion of each training epoch, measure the model's\n",
        "        # performance on our validation set.\n",
        "        val_loss, val_accuracy, val_precision, val_recall, val_auc, val_ap = evaluate(model, loader_val, loss_fn)\n",
        "\n",
        "        # Print performance over the entire training data\n",
        "        if tuning:\n",
        "          with tune.checkpoint_dir(epoch_i) as checkpoint_dir:\n",
        "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
        "            torch.save((model.state_dict(), optimizer.state_dict()), path)\n",
        "          tune.report(loss = val_loss, precision = val_precision, recall = val_recall, auc = val_auc, ap = val_ap)\n",
        "        else:\n",
        "          print(f\"{epoch_i + 1:^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_precision:^10.2f} | {val_recall:^10.2f} | {val_auc:^10.4f} | {val_ap:^10.4f}\")\n",
        "            \n",
        "        \n",
        "        # # =======================================\n",
        "        # #               Checkpoint\n",
        "        # # =======================================\n",
        "\n",
        "        # torch.save(model.state_dict(), f\"model_{time}_{epoch_i}.pth\")\n",
        "\n",
        "        gc.collect()\n",
        "\n",
        "    print(f\"Training complete!\")\n",
        "    if not tuning:\n",
        "      return model"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLeCWHxmKFAb"
      },
      "source": [
        "def evaluate(model, val_dataloader, loss_fn = nn.CrossEntropyLoss):\n",
        "    \"\"\"Measure a model's performance on a validation set.\n",
        "    Args:\n",
        "      model: a model object to evaluate.\n",
        "      val_dataloader: a dataloader with validation data.\n",
        "      loss_fn: a loss function to calculate the validation loss with. \n",
        "        Usually passed on within train() to be the same loss function as used for training.\n",
        "        Default = nn.CrossEntropyLoss, but should be overwritten to match training loss_fn.\n",
        "    Returns:\n",
        "      val_loss: the mean of the loss across batches.\n",
        "      val_accuracy: the mean of the accuracy (correct predictions based on cutoff 0.5) across batches.\n",
        "      val_precision: the fraction of correct positive predictions based on cutoff 0.5 (also known as positive predictive value)\n",
        "      val_recall: the fraction of actual positives that were predicted by the model based on cutoff 0.5 (also known as sensitivity or true positive rate)\n",
        "      val_auc: the area under the ROC curve, indicates our model's capability to distinguish between the two classes.\n",
        "      val_ap: the average precision, aka the area under the precision-recall curve. indicates our model's capability to distinguish the positive values correctly.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled\n",
        "    # during the test time.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    val_loss = []\n",
        "    true_labs_all = []\n",
        "    probs_all = []\n",
        "    preds_all = []\n",
        "\n",
        "    # For each batch in our validation set...\n",
        "    for batch in val_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Compute scores (shape: (batch, n_classes, length))\n",
        "        with torch.no_grad():\n",
        "            scores = model(b_input.float())\n",
        "\n",
        "        # Compute loss\n",
        "        if loss_fn is not None:\n",
        "          loss = loss_fn(scores, b_labels)\n",
        "          val_loss.append(loss.item())\n",
        "\n",
        "        # Get the probabilities and predictions\n",
        "        true_labs = b_labels.cpu().numpy()\n",
        "        probs = F.softmax(scores, dim=1).cpu().numpy()[:, 1] # keep only probabilities for label 1\n",
        "        preds = torch.argmax(scores, dim=1).cpu().numpy()\n",
        "\n",
        "        # Save to compute AUC and average precision (from precision-recall curve) later\n",
        "        true_labs_all.append(true_labs.flatten())\n",
        "        probs_all.append(probs.flatten())\n",
        "        preds_all.append(preds.flatten())\n",
        "\n",
        "    # Compute the performance statistics over the entire test set\n",
        "    true_labs_all = np.hstack(true_labs_all)\n",
        "    probs_all = np.hstack(probs_all)\n",
        "    preds_all = np.hstack(preds_all)\n",
        "\n",
        "    if loss_fn is not None:\n",
        "      val_loss = np.mean(val_loss)\n",
        "    val_accuracy = (preds_all == true_labs_all).mean() * 100\n",
        "    val_precision = (preds_all[preds_all == 1] == true_labs_all[preds_all == 1]).mean()*100       # Also known as positive predictive value\n",
        "    val_recall = (preds_all[true_labs_all == 1] == true_labs_all[true_labs_all == 1]).mean()*100  # Also known as sensitivity or true positive rate\n",
        "    val_auc = roc_auc_score(y_true = true_labs_all, y_score = probs_all)                          # Area under ROC curve\n",
        "    val_ap = average_precision_score(y_true = true_labs_all, y_score = probs_all)                 # Area under position-recall curve\n",
        "\n",
        "    return val_loss, val_accuracy, val_precision, val_recall, val_auc, val_ap"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xi4WU27lkZXG"
      },
      "source": [
        "def tune_model(config, device, num_samples):\n",
        "  \"\"\"A function to tune models to find the best hyperparameters using ray.tune.\n",
        "  Args:\n",
        "    config: a configuration dictionary with tune search space indicators.\n",
        "    device: a pytorch device indicating whether the model should be loaded into cpu or gpu.\n",
        "  Returns:\n",
        "    A tune.ExperimentAnalysis object with information about the best trial. \n",
        "    Can be used in build_best_model() to reconstitute the model.\"\"\"\n",
        "\n",
        "  # Start training/tuning\n",
        "  scheduler = ASHAScheduler(\n",
        "      metric = \"ap\", # alternative: loss, min\n",
        "      mode = \"max\",\n",
        "      max_t = 20,\n",
        "      grace_period = 3,\n",
        "      reduction_factor = 2)\n",
        "  reporter = JupyterNotebookReporter(\n",
        "      overwrite = True,\n",
        "      metric_columns = [\"loss\", \"auc\", \"ap\", \"precision\", \"recall\", \"training_iteration\"])\n",
        "  search_alg = HyperOptSearch(\n",
        "      metric = \"ap\",\n",
        "      mode = \"max\")\n",
        "\n",
        "  result = tune.run(\n",
        "      partial(train, device = device, zip_path = zip_path, info = info_trainval, epochs = 20, tuning = True),\n",
        "      resources_per_trial = {\"gpu\": 1},\n",
        "      config = config,\n",
        "      num_samples = num_samples,\n",
        "      search_alg = search_alg,\n",
        "      scheduler = scheduler,\n",
        "      progress_reporter = reporter)\n",
        "\n",
        "  return result\n",
        "\n",
        "\n",
        "def build_best_model(best_trial, device):\n",
        "  best_trained_model = Net()\n",
        "  best_trained_model.to(device)\n",
        "\n",
        "  best_checkpoint_dir = best_trial.checkpoint.value\n",
        "  model_state, optimizer_state = torch.load(os.path.join(best_checkpoint_dir, \"checkpoint\"))\n",
        "  best_trained_model.load_state_dict(model_state)\n",
        "  return best_trained_model"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqfxuEaUNS9j"
      },
      "source": [
        "<b>3 - option a. Tune the model</b>\n",
        "\n",
        "This option will train the model `num_samples` times, trying different combinations of hyperparameters each time, and return the best one.  \n",
        "\n",
        "`num_samples` has different interactions with random parameter selections (like `tune.choice()` or `tune.loguniform()`) and grid search. One unit of `num_samples` leads to only one sample from all random parameters, but one full grid search of all parameters (i.e. `num_workers` = 1 with `tune.grid_search(['A', 'B', 'C'])` makes three trials!). For an explanation on how exactly `num_samples` works, see [here](https://docs.ray.io/en/latest/tune/api_docs/search_space.html).  \n",
        "\n",
        "\n",
        "For a non-tuning option, see 3b."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Agz1MjLfvpG5"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Select hyperparameters\n",
        "config = {\n",
        "    \"loss_balance_factor\": tune.choice([0.75, 1, 1.25]),\n",
        "    \"lr\": 0.001}\n",
        "\n",
        "# Tune model\n",
        "best_trial = tune_model(config, device, num_samples = 10) # Num_samples to # desired runs (like 10) if random choices, to 1 if grid search (will run entire grid once)\n",
        "\n",
        "# Reconstitute best model\n",
        "model = build_best_model(best_trial, device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nppYbtIiO-3Z"
      },
      "source": [
        "**3 - option b: Train without tuning**\n",
        "\n",
        "Run a simple 20-epoch training sequence with set parameters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjenIAgG3AIR",
        "outputId": "cbcb1cb7-367c-4d48-df22-93364781191e"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "config = {\n",
        "    \"loss_balance_factor\": None, \n",
        "    \"lr\": 0.001}\n",
        "model = train(config, device, zip_path, info_trainval, epochs = 20, tuning = False)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training...\n",
            "\n",
            " Epoch  |  Train Loss  |  Val Loss  | Prec. @.5  | Recall@.5  |    AUC     |     AP    \n",
            "---------------------------------------------------------------------------------------\n",
            "   1    |   0.677379   |  0.662747  |   62.02    |   100.00   |   0.5950   |   0.7311  \n",
            "   2    |   0.624037   |  0.580640  |   62.02    |   100.00   |   0.5948   |   0.7320  \n",
            "   3    |   0.537027   |  0.595621  |   62.02    |   100.00   |   0.6139   |   0.7463  \n",
            "   4    |   0.530222   |  0.866820  |   62.02    |   100.00   |   0.6338   |   0.7609  \n",
            "   5    |   0.561994   |  0.842624  |   62.50    |   100.00   |   0.6557   |   0.7781  \n",
            "   6    |   0.487188   |  0.761770  |   66.07    |   92.50    |   0.6733   |   0.7880  \n",
            "   7    |   0.509214   |  0.539709  |   66.97    |   91.25    |   0.6792   |   0.7954  \n",
            "   8    |   0.457603   |  0.452706  |   66.36    |   91.25    |   0.6864   |   0.8011  \n",
            "   9    |   0.495160   |  0.434785  |   66.36    |   88.75    |   0.6874   |   0.8024  \n",
            "  10    |   0.449436   |  0.428209  |   67.00    |   83.75    |   0.6930   |   0.8068  \n",
            "  11    |   0.451204   |  0.621427  |   67.33    |   85.00    |   0.6912   |   0.8060  \n",
            "  12    |   0.486243   |  0.455309  |   66.33    |   81.25    |   0.6894   |   0.8047  \n",
            "  13    |   0.442836   |  1.370366  |   65.49    |   92.50    |   0.6917   |   0.8061  \n",
            "  14    |   0.454547   |  0.458506  |   65.77    |   91.25    |   0.7019   |   0.8132  \n",
            "  15    |   0.468989   |  0.563386  |   66.97    |   91.25    |   0.6986   |   0.8124  \n",
            "  16    |   0.450250   |  0.674334  |   66.67    |   90.00    |   0.6971   |   0.8105  \n",
            "  17    |   0.450329   |  0.590913  |   69.89    |   81.25    |   0.7080   |   0.8162  \n",
            "  18    |   0.418891   |  0.436518  |   68.04    |   82.50    |   0.7011   |   0.8107  \n",
            "  19    |   0.492488   |  0.605316  |   67.00    |   83.75    |   0.6945   |   0.8072  \n",
            "  20    |   0.510601   |  0.600196  |   68.67    |   71.25    |   0.6902   |   0.8075  \n",
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVtP06ulzB8D"
      },
      "source": [
        "**4. Check performance on the test set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QT1bef-Hs_LG",
        "outputId": "ebad2b31-9b8a-49c7-fe4a-4e495d292af8"
      },
      "source": [
        "# print(best_trial.)\n",
        "\n",
        "# # Show best trial\n",
        "# best_trial = result.get_best_trial(\"loss\", \"min\", \"last-5-avg\")\n",
        "print(f\"Best trial config: {best_trial.config}\")\n",
        "print(f\"Best trial final validation loss: {best_trial.last_result['loss']}\")\n",
        "print(f\"Best trial final validation average precision: {best_trial.last_result['ap']}\")\n",
        "print(f\"Best trial final validation auc: {best_trial.last_result['auc']}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best trial config: {'kernel_size': 13, 'beta': 0.9999538920913715, 'lr': 0.001}\n",
            "Best trial final validation loss: 0.019430930105348427\n",
            "Best trial final validation average precision: 0.19278001602192688\n",
            "Best trial final validation auc: 0.9426880381350121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdHhnVyJKau7",
        "outputId": "11645b8f-6108-473e-a884-096a610168b5"
      },
      "source": [
        "# Build the test set\n",
        "data_test = EmbeddingDataset(zip_path, info_test)\n",
        "loader_test = DataLoader(data_test, **{'batch_size': 16, 'shuffle': True})\n",
        "\n",
        "# Run the best model on the test set\n",
        "test_loss, test_accuracy, test_precision, test_recall, test_auc, test_ap = evaluate(model, loader_test, loss_fn=None)\n",
        "print(f\"Test AUC: {test_auc:.4f} | Test AP:  {test_ap:.4f} | Test precision (cutoff=0.5): {test_precision:.2f} | Test recall (cutoff=0.5): {test_recall:.2f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test AUC: 0.9754 | Test AP:  0.2151 \n",
            " Test precision (cutoff=0.5): 17.53 | Test recall (cutoff=0.5): 57.56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YojIJ3DUMlur"
      },
      "source": [
        "<b>5. Save and export the model</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoR_FZVFEocN"
      },
      "source": [
        "torch.save(model.state_dict(), \"model_params.pth\")\n",
        "# torch.save(model, \"model_full.pth\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qvl7j9prEpi3"
      },
      "source": [
        "!cp model_params.pth /content/drive/MyDrive/NetOGlyc/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyuaNZBsiQiV"
      },
      "source": [
        "drive.flush_and_unmount()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
