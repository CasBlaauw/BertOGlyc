{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BertOGlyc-train-CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/casblaauw/BertOGlyc/blob/main/BertOGlyc_train_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3Us07TngRPW"
      },
      "source": [
        "Initial model architecture based on [Elnaggar et al. (2020)](https://www.biorxiv.org/content/10.1101/2020.07.12.199554v3.full) and [Heinzinger et al. (2019)](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-3220-8).  \n",
        "Data loader structure inspired by [this pytorch documentation](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html), \n",
        "model architecture inspired by [this pytorch documentation](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html), training loop inspired by [this CNN tutorial](https://chriskhanhtran.github.io/posts/cnn-sentence-classification/), weights for loss function inspired by [this tutorial](https://towardsdatascience.com/handling-class-imbalanced-data-using-a-loss-specifically-made-for-it-6e58fd65ffab), based on [this paper](https://arxiv.org/abs/1901.05555). Hyperparameter tuning based on [this tutorial](https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html) and general [Ray docs](https://docs.ray.io/en/latest/tune/key-concepts.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vxl162CygIT1"
      },
      "source": [
        "<b>0. Import functions</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILN5pWgQsKts",
        "outputId": "5ec2a4ed-0ddc-4fb6-ca92-1515a810194c"
      },
      "source": [
        "!pip install ray\n",
        "!pip install -U hyperopt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ray\n",
            "  Downloading ray-1.6.0-cp37-cp37m-manylinux2014_x86_64.whl (49.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 49.6 MB 6.4 kB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from ray) (3.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray) (3.0.12)\n",
            "Requirement already satisfied: protobuf>=3.15.3 in /usr/local/lib/python3.7/dist-packages (from ray) (3.17.3)\n",
            "Collecting redis>=3.5.0\n",
            "  Downloading redis-3.5.3-py2.py3-none-any.whl (72 kB)\n",
            "\u001b[K     |████████████████████████████████| 72 kB 576 kB/s \n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from ray) (7.1.2)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from ray) (21.2.0)\n",
            "Requirement already satisfied: grpcio>=1.28.1 in /usr/local/lib/python3.7/dist-packages (from ray) (1.39.0)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from ray) (1.19.5)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray) (1.0.2)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio>=1.28.1->ray) (1.15.0)\n",
            "Installing collected packages: redis, ray\n",
            "Successfully installed ray-1.6.0 redis-3.5.3\n",
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.7/dist-packages (0.1.2)\n",
            "Collecting hyperopt\n",
            "  Downloading hyperopt-0.2.5-py2.py3-none-any.whl (965 kB)\n",
            "\u001b[K     |████████████████████████████████| 965 kB 13.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from hyperopt) (4.62.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from hyperopt) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from hyperopt) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from hyperopt) (0.16.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from hyperopt) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from hyperopt) (1.4.1)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.7/dist-packages (from hyperopt) (2.6.2)\n",
            "Installing collected packages: hyperopt\n",
            "  Attempting uninstall: hyperopt\n",
            "    Found existing installation: hyperopt 0.1.2\n",
            "    Uninstalling hyperopt-0.1.2:\n",
            "      Successfully uninstalled hyperopt-0.1.2\n",
            "Successfully installed hyperopt-0.2.5\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.4-py2.py3-none-any.whl (124 kB)\n",
            "\u001b[K     |████████████████████████████████| 124 kB 12.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (1.15.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5E6UoRvWgIAC"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from ray import tune\n",
        "from ray.tune import JupyterNotebookReporter\n",
        "from ray.tune.schedulers import ASHAScheduler\n",
        "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
        "from functools import partial\n",
        "\n",
        "import os\n",
        "from google.colab import files, drive\n",
        "import gc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJt2CPHWQhzM",
        "outputId": "e878902e-3baa-4b22-9a36-05db68016bcf"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxhN3RM5gMxy"
      },
      "source": [
        "<b>1. Read in and pre-split data </b>\n",
        "\n",
        "The info data frame determines the contents of the dataset. Indexing the dataset (as done by the data loader) looks at the supplied info file, gets the gene/protein ID associated with that position, and retrieves that protein's embeddings from the zip file.   \n",
        "\n",
        "Therefore, the data can be split by simply splitting the info data frame and building EmbeddingDatasets/DataLoaders with those. The actual construction of those happens within the training function (to split into random train/validation sets for each training run) and at post-training testing, but the info dataframes are already prepared here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbnGST746lcM"
      },
      "source": [
        "# Define the Dataset class for use with DataLoader, reading in files as needed\n",
        "class EmbeddingDataset(Dataset):\n",
        "    \"\"\"Dataset of embeddings from ProtBert.\n",
        "    Path is expected to be a path to an zip/npz file containing the .npy arrays for each gene.\n",
        "    Then indexes into that zip file to find the f\"embeddings_{gene_id} file.\"\n",
        "    Info is expected to be a pandas dataframe with gene names as keys, 'sequence' and 'label' keys as lists/iterables.\"\"\"\n",
        "    def __init__(self, path, info):\n",
        "        self.path = path\n",
        "        self.info = info.reset_index() \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.info)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        with np.load(self.path) as zip:\n",
        "          embed = zip[f\"embeddings_{self.info['gene'][idx]}\"] \n",
        "          embed = embed.T # Need to return transposed because conv1d expects channels, then length\n",
        "        label = torch.tensor(self.info['label'][idx])\n",
        "        return embed, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqkwO3P1gMCH"
      },
      "source": [
        "# Define the paths to the info and zip file\n",
        "zip_path = '/content/drive/MyDrive/NetOGlyc/embeddings_npy.zip'\n",
        "info_path = '/content/drive/MyDrive/NetOGlyc/embeddings_info.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtPkQ4poa9L_",
        "outputId": "6cedf5dc-7f2a-4117-e4d8-95e9f19637f5"
      },
      "source": [
        "# Read in the info file\n",
        "info = pd.read_csv(info_path, sep = '\\t')\n",
        "info['sequence'] = info['sequence'].apply(list)\n",
        "info['label'] = info['label'].apply(lambda x: list(map(int, list(x))))\n",
        "\n",
        "# Split data into test and training files\n",
        "trainval_idx, test_idx = train_test_split(range(len(info)), test_size = 0.2)\n",
        "\n",
        "info_trainval = info.iloc[trainval_idx, :]\n",
        "info_test = info.iloc[test_idx, :]\n",
        "print(info_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           gene  ...                                              label\n",
            "493  O60487_neg  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "401  Q9H741_pos  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "530  P05067_neg  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "772  Q96AB3_neg  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "732  Q8N114_neg  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "..          ...  ...                                                ...\n",
            "253  Q5SRI9_pos  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "205  Q06481_pos  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "368  Q99075_pos  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "30   O60353_pos  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "380  Q9BS26_pos  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "\n",
            "[173 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3Dpd6ktgTJv"
      },
      "source": [
        "<b>2. Define model and function for weights</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbgX1XkJgXQ1"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, kernel_size = 7):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels = 1024, out_channels = 32, kernel_size = kernel_size, padding = kernel_size//2) \n",
        "        self.dropout = nn.Dropout(p=0.25)\n",
        "        self.conv2 = nn.Conv1d(in_channels = 32, out_channels = 2, kernel_size = kernel_size, padding = kernel_size//2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # ---- Layer 1\n",
        "        # conv1 needs (batch_size, in_channels/features, length/seq_len), so (64, 1024, 4000) \n",
        "        # and outputs (64, 32, 4000)\n",
        "        x = self.conv1(x)\n",
        "\n",
        "        # ---- Process first layer's output\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # ---- Layer 2\n",
        "        # conv2 takes (64, 32, 4000) and outputs (64, 2, 4000)\n",
        "        x = self.conv2(x)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcNVBOMb1hfo"
      },
      "source": [
        "# Loss weights can help prioritise properly predicting glycosites over the bulk unglycosylated sites\n",
        "# Priority here is predicting sites, not predicting non-sites, so we want a very high beta to distinguish between them still.\n",
        "\n",
        "# def cb_weights(labels, beta):\n",
        "#     \"\"\"Compute the weights for Class Balanced Loss.\n",
        "#     Class Balanced Loss: ((1-beta)/(1-beta^n))*Loss(labels, logits)\n",
        "#     where Loss is a loss function, here cross entropy loss.\n",
        "#     Args:\n",
        "#       labels: Pandas series of labels. Assumes binary 0/1 labeling.\n",
        "#       beta: float. Hyperparameter for Class balanced loss. \n",
        "#     Returns:\n",
        "#       A set of weights to be supplied to the loss function.\n",
        "#     \"\"\"\n",
        "#     total_samples = labels.apply(len).sum()\n",
        "#     total_sites = labels.apply(sum).sum() # Assumes 0-1 labeling\n",
        "#     samples_dist = [total_samples - total_sites, total_sites]\n",
        "\n",
        "#     effective_num = 1.0 - np.power(beta, samples_dist)\n",
        "#     weights = (1.0 - beta) / np.array(effective_num)\n",
        "#     weights = weights / np.sum(weights) * len(samples_dist)\n",
        "#     return weights\n",
        "    \n",
        "def weighted_weights(labels, balance_factor = 1):\n",
        "  \"\"\"Compute the weights for the loss function, weighted by proportion in the data.\n",
        "  Args:\n",
        "    labels: Pandas series of labels. Assumes binary 0/1 labeling.\n",
        "    balance_factor: float. Hyperparameter for loss balancing. Default is 1.\n",
        "      Increasing it above 1 will make 1-labels more important than their proportion in the data, and vice versa.\n",
        "  Returns:\n",
        "    A set of weights to be supplied to the loss function.\n",
        "  \"\"\"\n",
        "  if balance_factor is None:\n",
        "    return [1, 1]\n",
        "  else:\n",
        "    total_samples = labels.apply(len).sum()\n",
        "    total_sites = labels.apply(sum).sum() # Assumes 0-1 labeling\n",
        "    total_nonsites = total_samples - total_sites\n",
        "    weights = [1, balance_factor * (total_nonsites / total_sites)]\n",
        "    return weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anF-WMUyJ5Lj"
      },
      "source": [
        "**3. Define training and tuning functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UmeQMvnwixJ"
      },
      "source": [
        "def train(config, device, zip_path, info, epochs=10, tuning = False, checkpoint_dir = None):\n",
        "    \"\"\"Train the CNN model.\n",
        "    Args: \n",
        "      config: a dictionary with hyperparameter values {'kernel_size', 'loss_balance_factor', 'lr'}. \n",
        "        If tuning = True, supports ray.tune search spaces.\n",
        "      device: a pytorch device indicating whether the model should be loaded into cpu or gpu.\n",
        "      zip_path: a path to an zip/npz file containing the .npy arrays for each gene.\n",
        "      info: a pandas dataframe with gene names as keys, 'sequence' and 'label' keys as lists/iterables.\n",
        "      epochs: optional. an integer value, indicating the number of epochs (training loops) the training should last.\n",
        "      tuning: optional. a boolean indicating whether the model is ran in the context of ray.tune tuning.\n",
        "        In that case, it won't print training results, but will instead pass them to ray.tune.\n",
        "        Default = False.\n",
        "      checkpoint_dir: optional. only used when tuning = true. used to retrieve the best model's model_state after tuning.\n",
        "        Default = None.\n",
        "    Returns:\n",
        "      Doesn't return anything, but has modified the weights of the supplied model object. \n",
        "      \"\"\"\n",
        "\n",
        "    # Initialise model\n",
        "    model = Net(kernel_size = config['kernel_size'])\n",
        "    model.to(device)\n",
        "\n",
        "    # Load info and paths into dataset objects and create loaders\n",
        "    train_idx, val_idx = train_test_split(range(len(info)), test_size = 0.2)\n",
        "    loader_params = {'batch_size': 64, 'shuffle': True}\n",
        "\n",
        "    info_train = info.iloc[train_idx, :]\n",
        "    data_train = EmbeddingDataset(zip_path, info_train)\n",
        "    loader_train = DataLoader(data_train, **loader_params)\n",
        "\n",
        "    info_val = info.iloc[val_idx, :]\n",
        "    data_val = EmbeddingDataset(zip_path, info_val)\n",
        "    loader_val = DataLoader(data_val, **loader_params)\n",
        "\n",
        "    # Gather weights\n",
        "    weights = weighted_weights(info_train['label'], balance_factor = config['loss_balance_factor'])\n",
        "\n",
        "    # Initialise loss function and optimizer \n",
        "    loss_fn = nn.CrossEntropyLoss(weight = torch.FloatTensor(weights).to(device))\n",
        "    optimizer = optim.Adam(model.parameters(), lr = config['lr'], amsgrad = True) \n",
        "\n",
        "    if tuning and checkpoint_dir:\n",
        "        model_state, optimizer_state = torch.load(\n",
        "            os.path.join(checkpoint_dir, \"checkpoint\"))\n",
        "        model.load_state_dict(model_state)\n",
        "        optimizer.load_state_dict(optimizer_state)\n",
        "\n",
        "    # Start training loop\n",
        "    if not tuning:\n",
        "      print(\"Start training...\\n\")\n",
        "      print(f\"{'Epoch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Prec. @.5':^10} | {'Recall@.5':^10} | {'AUC':^10} | {'AP':^10}\")\n",
        "      print(\"-\"*87)\n",
        "\n",
        "    model = model.float()\n",
        "\n",
        "    for epoch_i in range(epochs):\n",
        "        # =======================================\n",
        "        #               Training\n",
        "        # =======================================\n",
        "\n",
        "        # Tracking time and loss\n",
        "        total_loss = 0\n",
        "\n",
        "        # Put the model into the training model\n",
        "        model.train()\n",
        "\n",
        "        for step, batch in enumerate(loader_train):\n",
        "            # Load batch to GPU\n",
        "            b_input_ids, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "            # Zero out any previously calculated gradients\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Perform a forward pass. (output shape: (batch, n_classes, length))\n",
        "            logits = model(b_input_ids.float())\n",
        "\n",
        "            # Compute loss and accumulate the loss values\n",
        "            loss = loss_fn(logits, b_labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Perform a backward pass to calculate gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Update parameters\n",
        "            optimizer.step()\n",
        "\n",
        "        # Calculate the average loss over the entire training data\n",
        "        avg_train_loss = total_loss / len(loader_train)\n",
        "\n",
        "        # =======================================\n",
        "        #               Evaluation\n",
        "        # =======================================\n",
        "        # After the completion of each training epoch, measure the model's\n",
        "        # performance on our validation set.\n",
        "        val_loss, val_accuracy, val_precision, val_recall, val_auc, val_ap = evaluate(model, loader_val, loss_fn)\n",
        "\n",
        "        # Print performance over the entire training data\n",
        "        if tuning:\n",
        "          with tune.checkpoint_dir(epoch_i) as checkpoint_dir:\n",
        "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
        "            torch.save((model.state_dict(), optimizer.state_dict()), path)\n",
        "          tune.report(loss = val_loss, precision = val_precision, recall = val_recall, auc = val_auc, ap = val_ap)\n",
        "        else:\n",
        "          print(f\"{epoch_i + 1:^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_precision:^10.2f} | {val_recall:^10.2f} | {val_auc:^10.4f} | {val_ap:^10.4f}\")\n",
        "            \n",
        "        \n",
        "        # # =======================================\n",
        "        # #               Checkpoint\n",
        "        # # =======================================\n",
        "\n",
        "        # torch.save(model.state_dict(), f\"model_{time}_{epoch_i}.pth\")\n",
        "\n",
        "        gc.collect()\n",
        "\n",
        "    print(f\"Training complete!\")\n",
        "    if not tuning:\n",
        "      return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLeCWHxmKFAb"
      },
      "source": [
        "def evaluate(model, val_dataloader, loss_fn = nn.CrossEntropyLoss):\n",
        "    \"\"\"Measure a model's performance on a validation set.\n",
        "    Args:\n",
        "      model: a model object to evaluate.\n",
        "      val_dataloader: a dataloader with validation data.\n",
        "      loss_fn: a loss function to calculate the validation loss with. \n",
        "        Usually passed on within train() to be the same loss function as used for training.\n",
        "        Default = nn.CrossEntropyLoss, but should be overwritten to match training loss_fn.\n",
        "    Returns:\n",
        "      val_loss: the mean of the loss across batches.\n",
        "      val_accuracy: the mean of the accuracy (correct predictions based on cutoff 0.5) across batches.\n",
        "      val_precision: the fraction of correct positive predictions based on cutoff 0.5 (also known as positive predictive value)\n",
        "      val_recall: the fraction of actual positives that were predicted by the model based on cutoff 0.5 (also known as sensitivity or true positive rate)\n",
        "      val_auc: the area under the ROC curve, indicates our model's capability to distinguish between the two classes.\n",
        "      val_ap: the average precision, aka the area under the precision-recall curve. indicates our model's capability to distinguish the positive values correctly.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled\n",
        "    # during the test time.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    val_loss = []\n",
        "    true_labs_all = []\n",
        "    probs_all = []\n",
        "    preds_all = []\n",
        "\n",
        "    # For each batch in our validation set...\n",
        "    for batch in val_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Compute scores (shape: (batch, n_classes, length))\n",
        "        with torch.no_grad():\n",
        "            scores = model(b_input.float())\n",
        "\n",
        "        # Compute loss\n",
        "        if loss_fn is not None:\n",
        "          loss = loss_fn(scores, b_labels)\n",
        "          val_loss.append(loss.item())\n",
        "\n",
        "        # Get the probabilities and predictions\n",
        "        true_labs = b_labels.cpu().numpy()\n",
        "        probs = F.softmax(scores, dim=1).cpu().numpy()[:, 1, :] # keep only probabilities for label 1\n",
        "        preds = torch.argmax(scores, dim=1).cpu().numpy()\n",
        "\n",
        "        # Save to compute AUC and average precision (from precision-recall curve) later\n",
        "        true_labs_all.append(true_labs.flatten())\n",
        "        probs_all.append(probs.flatten())\n",
        "        preds_all.append(preds.flatten())\n",
        "\n",
        "    # Compute the performance statistics over the entire test set\n",
        "    true_labs_all = np.hstack(true_labs_all)\n",
        "    probs_all = np.hstack(probs_all)\n",
        "    preds_all = np.hstack(preds_all)\n",
        "\n",
        "    if loss_fn is not None:\n",
        "      val_loss = np.mean(val_loss)\n",
        "    val_accuracy = (preds_all == true_labs_all).mean() * 100\n",
        "    val_precision = (preds_all[preds_all == 1] == true_labs_all[preds_all == 1]).mean()*100       # Also known as positive predictive value\n",
        "    val_recall = (preds_all[true_labs_all == 1] == true_labs_all[true_labs_all == 1]).mean()*100  # Also known as sensitivity or true positive rate\n",
        "    val_auc = roc_auc_score(y_true = true_labs_all, y_score = probs_all)                          # Area under ROC curve\n",
        "    val_ap = average_precision_score(y_true = true_labs_all, y_score = probs_all)                 # Area under position-recall curve\n",
        "\n",
        "    return val_loss, val_accuracy, val_precision, val_recall, val_auc, val_ap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xi4WU27lkZXG"
      },
      "source": [
        "def tune_model(config, device, num_samples):\n",
        "  \"\"\"A function to tune models to find the best hyperparameters using ray.tune.\n",
        "  Args:\n",
        "    config: a configuration dictionary with tune search space indicators.\n",
        "    device: a pytorch device indicating whether the model should be loaded into cpu or gpu.\n",
        "  Returns:\n",
        "    A tune.ExperimentAnalysis object with information about the best trial. \n",
        "    Can be used in build_best_model() to reconstitute the model.\"\"\"\n",
        "\n",
        "  # Start training/tuning\n",
        "  scheduler = ASHAScheduler(\n",
        "      metric = \"ap\", # alternative: loss, min\n",
        "      mode = \"max\",\n",
        "      max_t = 20,\n",
        "      grace_period = 3,\n",
        "      reduction_factor = 2)\n",
        "  reporter = JupyterNotebookReporter(\n",
        "      overwrite = True,\n",
        "      metric_columns = [\"loss\", \"auc\", \"ap\", \"precision\", \"recall\", \"training_iteration\"])\n",
        "  search_alg = HyperOptSearch(\n",
        "      metric = \"ap\",\n",
        "      mode = \"max\")\n",
        "\n",
        "  result = tune.run(\n",
        "      partial(train, device = device, zip_path = zip_path, info = info_trainval, epochs = 20, tuning = True),\n",
        "      resources_per_trial = {\"gpu\": 1},\n",
        "      config = config,\n",
        "      num_samples = num_samples,\n",
        "      search_alg = search_alg,\n",
        "      scheduler = scheduler,\n",
        "      progress_reporter = reporter)\n",
        "\n",
        "  return result\n",
        "\n",
        "\n",
        "def build_best_model(best_trial, device):\n",
        "  best_trained_model = Net(kernel_size = best_trial.config[\"kernel_size\"])\n",
        "  best_trained_model.to(device)\n",
        "\n",
        "  best_checkpoint_dir = best_trial.checkpoint.value\n",
        "  model_state, optimizer_state = torch.load(os.path.join(best_checkpoint_dir, \"checkpoint\"))\n",
        "  best_trained_model.load_state_dict(model_state)\n",
        "  return best_trained_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqfxuEaUNS9j"
      },
      "source": [
        "<b>3 - option a. Tune the model</b>\n",
        "\n",
        "This option will train the model `num_samples` times, trying different combinations of hyperparameters each time, and return the best one.  \n",
        "\n",
        "`num_samples` has different interactions with random parameter selections (like `tune.choice()` or `tune.loguniform()`) and grid search. One unit of `num_samples` leads to only one sample from all random parameters, but one full grid search of all parameters (i.e. `num_workers` = 1 with `tune.grid_search(['A', 'B', 'C'])` makes three trials!). For an explanation on how exactly `num_samples` works, see [here](https://docs.ray.io/en/latest/tune/api_docs/search_space.html).  \n",
        "\n",
        "\n",
        "For a non-tuning option, see 3b."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "id": "Agz1MjLfvpG5",
        "outputId": "d22810a1-741b-42cd-9c89-b8e100ad2056"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Select hyperparameters\n",
        "config = {\n",
        "    \"kernel_size\": tune.choice([5, 7, 9, 13]),\n",
        "    \"loss_balance_factor\": tune.choice([0.75, 1, 1.25]),\n",
        "    \"lr\": 0.001}\n",
        "\n",
        "# Tune model\n",
        "best_trial = tune_model(config, device, num_samples = 10) # Num_samples to # desired runs (like 10) if random choices, to 1 if grid search (will run entire grid once)\n",
        "\n",
        "# Reconstitute best model\n",
        "model = build_best_model(best_trial, device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 3.1/12.7 GiB<br>Using AsyncHyperBand: num_stopped=10\n",
              "Bracket: Iter 12.000: 0.10036793275573325 | Iter 6.000: 0.08741672207589907 | Iter 3.000: 0.03821025901460351<br>Resources requested: 0/2 CPUs, 0/1 GPUs, 0.0/7.33 GiB heap, 0.0/3.66 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/DEFAULT_2021-09-11_15-30-52<br>Number of trials: 10/10 (10 TERMINATED)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name      </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">    beta</th><th style=\"text-align: right;\">  kernel_size</th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">     loss</th><th style=\"text-align: right;\">     auc</th><th style=\"text-align: right;\">         ap</th><th style=\"text-align: right;\">  precision</th><th style=\"text-align: right;\">  recall</th><th style=\"text-align: right;\">  training_iteration</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>DEFAULT_3f9cc9ee</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.999937</td><td style=\"text-align: right;\">            5</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.0339966</td><td style=\"text-align: right;\">0.926077</td><td style=\"text-align: right;\">0.133444   </td><td style=\"text-align: right;\">  nan      </td><td style=\"text-align: right;\">  0     </td><td style=\"text-align: right;\">                  20</td></tr>\n",
              "<tr><td>DEFAULT_40536f1e</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.999943</td><td style=\"text-align: right;\">            5</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.0655836</td><td style=\"text-align: right;\">0.903175</td><td style=\"text-align: right;\">0.0705934  </td><td style=\"text-align: right;\">  nan      </td><td style=\"text-align: right;\">  0     </td><td style=\"text-align: right;\">                  12</td></tr>\n",
              "<tr><td>DEFAULT_1e759c2e</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.999955</td><td style=\"text-align: right;\">            5</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.0947441</td><td style=\"text-align: right;\">0.921972</td><td style=\"text-align: right;\">0.0669419  </td><td style=\"text-align: right;\">  nan      </td><td style=\"text-align: right;\">  0     </td><td style=\"text-align: right;\">                  12</td></tr>\n",
              "<tr><td>DEFAULT_1cfe895a</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.999968</td><td style=\"text-align: right;\">           13</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.023049 </td><td style=\"text-align: right;\">0.970192</td><td style=\"text-align: right;\">0.204706   </td><td style=\"text-align: right;\">   18.1924 </td><td style=\"text-align: right;\"> 55.0877</td><td style=\"text-align: right;\">                  20</td></tr>\n",
              "<tr><td>DEFAULT_6542918a</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.999959</td><td style=\"text-align: right;\">            9</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.0540958</td><td style=\"text-align: right;\">0.927753</td><td style=\"text-align: right;\">0.0818549  </td><td style=\"text-align: right;\">  nan      </td><td style=\"text-align: right;\">  0     </td><td style=\"text-align: right;\">                  12</td></tr>\n",
              "<tr><td>DEFAULT_0b863e38</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.999988</td><td style=\"text-align: right;\">           13</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.0825802</td><td style=\"text-align: right;\">0.94304 </td><td style=\"text-align: right;\">0.191116   </td><td style=\"text-align: right;\">   14.5583 </td><td style=\"text-align: right;\"> 64.375 </td><td style=\"text-align: right;\">                  20</td></tr>\n",
              "<tr><td>DEFAULT_a407e676</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.999934</td><td style=\"text-align: right;\">           13</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.492432 </td><td style=\"text-align: right;\">0.159278</td><td style=\"text-align: right;\">0.000369411</td><td style=\"text-align: right;\">  nan      </td><td style=\"text-align: right;\">  0     </td><td style=\"text-align: right;\">                   3</td></tr>\n",
              "<tr><td>DEFAULT_51bea394</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.99991 </td><td style=\"text-align: right;\">            5</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.528582 </td><td style=\"text-align: right;\">0.150778</td><td style=\"text-align: right;\">0.000337385</td><td style=\"text-align: right;\">  nan      </td><td style=\"text-align: right;\">  0     </td><td style=\"text-align: right;\">                   3</td></tr>\n",
              "<tr><td>DEFAULT_5bec40f0</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.999954</td><td style=\"text-align: right;\">           13</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.0194309</td><td style=\"text-align: right;\">0.942688</td><td style=\"text-align: right;\">0.19278    </td><td style=\"text-align: right;\">   16.9394 </td><td style=\"text-align: right;\"> 57.8947</td><td style=\"text-align: right;\">                  20</td></tr>\n",
              "<tr><td>DEFAULT_63d39f38</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.999987</td><td style=\"text-align: right;\">           13</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.238874 </td><td style=\"text-align: right;\">0.826304</td><td style=\"text-align: right;\">0.0543146  </td><td style=\"text-align: right;\">    5.71319</td><td style=\"text-align: right;\"> 64.2241</td><td style=\"text-align: right;\">                   6</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-09-11 21:05:22,064\tINFO tune.py:561 -- Total run time: 20070.12 seconds (20069.31 seconds for the tuning loop).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best trial config: {'kernel_size': 13, 'beta': 0.9999538920913715, 'lr': 0.001}\n",
            "Best trial final validation loss: 0.019430930105348427\n",
            "Best trial final validation average precision: 0.19278001602192688\n",
            "Best trial final validation auc: 0.9426880381350121\n",
            "2021-09-11 21:05:43.289042: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-11 21:05:43.893115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-11 21:05:43.894052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "\n",
            "NOTE: Using experimental fast data loading logic. To disable, pass\n",
            "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
            "    https://github.com/tensorflow/tensorboard/issues/4784\n",
            "\n",
            "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
            "TensorBoard 2.6.0 at http://localhost:6006/ (Press CTRL+C to quit)\n",
            "^C\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-09-11 21:08:53,423\tWARNING tune.py:519 -- SIGINT received (e.g. via Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C one more time (or send SIGINT/SIGKILL/SIGTERM) to skip. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nppYbtIiO-3Z"
      },
      "source": [
        "**3 - option b: Train without tuning**\n",
        "\n",
        "Run a simple 20-epoch training sequence with set parameters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjenIAgG3AIR",
        "outputId": "a8916d7c-3a40-41f8-d22a-293c2e2b9f58"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "config = {\n",
        "    \"kernel_size\": 13,\n",
        "    \"loss_balance_factor\": 1, \n",
        "    \"lr\": 0.001}\n",
        "model = train(config, device, zip_path, info_trainval, epochs = 20, tuning = False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training...\n",
            "\n",
            " Epoch  |  Train Loss  |  Val Loss  | Prec. @.5  | Recall@.5  |    AUC     |     AP    \n",
            "---------------------------------------------------------------------------------------\n",
            "   1    |   0.507909   |  0.349312  |     0.69     |    100.00    |   0.9919   |   0.1136  \n",
            "   2    |   0.402152   |  0.347138  |     1.08     |    99.76     |   0.9947   |   0.1331  \n",
            "   3    |   0.353733   |  0.258227  |     2.48     |    98.55     |   0.9844   |   0.1519  \n",
            "   4    |   0.308034   |  0.262455  |     2.63     |    99.28     |   0.9928   |   0.1614  \n",
            "   5    |   0.272004   |  0.226134  |     3.37     |    99.03     |   0.9911   |   0.1741  \n",
            "   6    |   0.237626   |  0.196152  |     4.51     |    98.07     |   0.9871   |   0.1764  \n",
            "   7    |   0.201981   |  0.165623  |     5.12     |    98.07     |   0.9852   |   0.1831  \n",
            "   8    |   0.164654   |  0.154746  |     4.72     |    98.07     |   0.9892   |   0.1883  \n",
            "   9    |   0.132409   |  0.130325  |     5.16     |    98.07     |   0.9892   |   0.2006  \n",
            "  10    |   0.102198   |  0.128168  |     5.54     |    97.83     |   0.9850   |   0.1922  \n",
            "  11    |   0.077759   |  0.098543  |     5.34     |    97.58     |   0.9911   |   0.1971  \n",
            "  12    |   0.063246   |  0.107226  |     6.37     |    96.62     |   0.9872   |   0.1941  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVtP06ulzB8D"
      },
      "source": [
        "**4. Check performance on the test set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QT1bef-Hs_LG",
        "outputId": "ebad2b31-9b8a-49c7-fe4a-4e495d292af8"
      },
      "source": [
        "# print(best_trial.)\n",
        "\n",
        "# # Show best trial\n",
        "# best_trial = result.get_best_trial(\"loss\", \"min\", \"last-5-avg\")\n",
        "print(f\"Best trial config: {best_trial.config}\")\n",
        "print(f\"Best trial final validation loss: {best_trial.last_result['loss']}\")\n",
        "print(f\"Best trial final validation average precision: {best_trial.last_result['ap']}\")\n",
        "print(f\"Best trial final validation auc: {best_trial.last_result['auc']}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best trial config: {'kernel_size': 13, 'beta': 0.9999538920913715, 'lr': 0.001}\n",
            "Best trial final validation loss: 0.019430930105348427\n",
            "Best trial final validation average precision: 0.19278001602192688\n",
            "Best trial final validation auc: 0.9426880381350121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdHhnVyJKau7",
        "outputId": "11645b8f-6108-473e-a884-096a610168b5"
      },
      "source": [
        "# Build the test set\n",
        "data_test = EmbeddingDataset(zip_path, info_test)\n",
        "loader_test = DataLoader(data_test, **{'batch_size': 16, 'shuffle': True})\n",
        "\n",
        "# Run the best model on the test set\n",
        "test_loss, test_accuracy, test_precision, test_recall, test_auc, test_ap = evaluate(model, loader_test, loss_fn=None)\n",
        "print(f\"Test AUC: {test_auc:.4f} | Test AP:  {test_ap:.4f} | Test precision (cutoff=0.5): {test_precision:.2f} | Test recall (cutoff=0.5): {test_recall:.2f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test AUC: 0.9754 | Test AP:  0.2151 \n",
            " Test precision (cutoff=0.5): 17.53 | Test recall (cutoff=0.5): 57.56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YojIJ3DUMlur"
      },
      "source": [
        "<b>5. Save and export the model</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoR_FZVFEocN"
      },
      "source": [
        "torch.save(model.state_dict(), \"model_params.pth\")\n",
        "# torch.save(model, \"model_full.pth\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qvl7j9prEpi3"
      },
      "source": [
        "!cp model_params.pth /content/drive/MyDrive/NetOGlyc/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyuaNZBsiQiV"
      },
      "source": [
        "drive.flush_and_unmount()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}