{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ProtBert-NetOGlyc-classification-training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPdgsCXDSBhmfKw36wuuAbg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/casblaauw/BertOGlyc/blob/main/ProtBert_NetOGlyc_classification_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3Us07TngRPW"
      },
      "source": [
        "Initial model architecture based on [Elnaggar et al. (2020)](https://www.biorxiv.org/content/10.1101/2020.07.12.199554v3.full) and [Heinzinger et al. (2019)](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-3220-8).  \n",
        "Data loader structure inspired by [this pytorch documentation](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html), \n",
        "model architecture inspired by [this pytorch documentation](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html), training loop inspired by [this CNN tutorial](https://chriskhanhtran.github.io/posts/cnn-sentence-classification/), weights for loss function inspired by [this tutorial](https://towardsdatascience.com/handling-class-imbalanced-data-using-a-loss-specifically-made-for-it-6e58fd65ffab), based on [this paper](https://arxiv.org/abs/1901.05555). Hyperparameter tuning based on [this tutorial](https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html) and general [Ray docs](https://docs.ray.io/en/latest/tune/key-concepts.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vxl162CygIT1"
      },
      "source": [
        "<b>0. Import functions</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILN5pWgQsKts",
        "outputId": "5ec2a4ed-0ddc-4fb6-ca92-1515a810194c"
      },
      "source": [
        "!pip install ray\n",
        "!pip install -U hyperopt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ray\n",
            "  Downloading ray-1.6.0-cp37-cp37m-manylinux2014_x86_64.whl (49.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 49.6 MB 6.4 kB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from ray) (3.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray) (3.0.12)\n",
            "Requirement already satisfied: protobuf>=3.15.3 in /usr/local/lib/python3.7/dist-packages (from ray) (3.17.3)\n",
            "Collecting redis>=3.5.0\n",
            "  Downloading redis-3.5.3-py2.py3-none-any.whl (72 kB)\n",
            "\u001b[K     |████████████████████████████████| 72 kB 576 kB/s \n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from ray) (7.1.2)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from ray) (21.2.0)\n",
            "Requirement already satisfied: grpcio>=1.28.1 in /usr/local/lib/python3.7/dist-packages (from ray) (1.39.0)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from ray) (1.19.5)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray) (1.0.2)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio>=1.28.1->ray) (1.15.0)\n",
            "Installing collected packages: redis, ray\n",
            "Successfully installed ray-1.6.0 redis-3.5.3\n",
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.7/dist-packages (0.1.2)\n",
            "Collecting hyperopt\n",
            "  Downloading hyperopt-0.2.5-py2.py3-none-any.whl (965 kB)\n",
            "\u001b[K     |████████████████████████████████| 965 kB 13.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from hyperopt) (4.62.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from hyperopt) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from hyperopt) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from hyperopt) (0.16.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from hyperopt) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from hyperopt) (1.4.1)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.7/dist-packages (from hyperopt) (2.6.2)\n",
            "Installing collected packages: hyperopt\n",
            "  Attempting uninstall: hyperopt\n",
            "    Found existing installation: hyperopt 0.1.2\n",
            "    Uninstalling hyperopt-0.1.2:\n",
            "      Successfully uninstalled hyperopt-0.1.2\n",
            "Successfully installed hyperopt-0.2.5\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.4-py2.py3-none-any.whl (124 kB)\n",
            "\u001b[K     |████████████████████████████████| 124 kB 12.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (1.15.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5E6UoRvWgIAC"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from ray import tune\n",
        "from ray.tune import JupyterNotebookReporter\n",
        "from ray.tune.schedulers import ASHAScheduler\n",
        "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
        "from functools import partial\n",
        "\n",
        "import os\n",
        "from google.colab import files, drive\n",
        "import gc"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJt2CPHWQhzM",
        "outputId": "ef90d103-8c08-4e59-cbed-781b27dbbb91"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxhN3RM5gMxy"
      },
      "source": [
        "<b>1. Read in and pre-split data </b>\n",
        "\n",
        "The info data frame determines the contents of the dataset. Indexing the dataset (as done by the data loader) looks at the supplied info file, gets the gene/protein ID associated with that position, and retrieves that protein's embeddings from the zip file.   \n",
        "\n",
        "Therefore, the data can be split by simply splitting the info data frame and building EmbeddingDatasets/DataLoaders with those. The actual construction of those happens within the training function (to split into random train/validation sets for each training run) and at post-training testing, but the info dataframes are already prepared here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbnGST746lcM"
      },
      "source": [
        "# Define the Dataset class for use with DataLoader, reading in files as needed\n",
        "class EmbeddingDataset(Dataset):\n",
        "    \"\"\"Dataset of embeddings from ProtBert.\n",
        "    Path is expected to be a path to an zip/npz file containing the .npy arrays for each gene.\n",
        "    Then indexes into that zip file to find the f\"embeddings_{gene_id} file.\"\n",
        "    Info is expected to be a pandas dataframe with gene names as keys, 'sequence' and 'label' keys as lists/iterables.\"\"\"\n",
        "    def __init__(self, path, info):\n",
        "        self.path = path\n",
        "        self.info = info.reset_index() \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.info)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        with np.load(self.path) as zip:\n",
        "          embed = zip[f\"embeddings_{self.info['gene'][idx]}\"] \n",
        "          embed = embed.T # Need to return transposed because conv1d expects channels, then length\n",
        "        label = torch.tensor(self.info['label'][idx])\n",
        "        return embed, label"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqkwO3P1gMCH"
      },
      "source": [
        "# Define the paths to the info and zip file\n",
        "zip_path = '/content/drive/MyDrive/NetOGlyc/embeddings_npy.zip'\n",
        "info_path = '/content/drive/MyDrive/NetOGlyc/embeddings_info.txt'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtPkQ4poa9L_",
        "outputId": "dfc9309e-92ad-483e-9f95-307265ef8e25"
      },
      "source": [
        "# Read in the info file\n",
        "info = pd.read_csv(info_path, sep = '\\t')\n",
        "info['sequence'] = info['sequence'].apply(list)\n",
        "info['label'] = info['label'].apply(lambda x: list(map(int, list(x))))\n",
        "\n",
        "# Split data into test and training files\n",
        "trainval_idx, test_idx = train_test_split(range(len(info)), test_size = 0.2)\n",
        "\n",
        "info_trainval = info.iloc[trainval_idx, :]\n",
        "info_test = info.iloc[test_idx, :]\n",
        "print(info_test)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           gene  ...                                              label\n",
            "90   P05362_pos  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "230  Q14767_pos  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "845  Q9UKU7_neg  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "663  Q04721_neg  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "488  O43278_neg  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "..          ...  ...                                                ...\n",
            "305  Q86TE4_pos  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "263  Q6E0U4_pos  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "560  P10619_neg  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "200  Q01974_pos  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "251  Q5JRA6_pos  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "\n",
            "[173 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3Dpd6ktgTJv"
      },
      "source": [
        "<b>2. Define model and function for weights</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbgX1XkJgXQ1"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, kernel_size = 7):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels = 1024, out_channels = 32, kernel_size = kernel_size, padding = kernel_size//2) \n",
        "        self.dropout = nn.Dropout(p=0.25)\n",
        "        self.conv2 = nn.Conv1d(in_channels = 32, out_channels = 2, kernel_size = kernel_size, padding = kernel_size//2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # ---- Layer 1\n",
        "        # conv1 needs (batch_size, in_channels/features, length/seq_len), so (64, 1024, 4000) \n",
        "        # and outputs (64, 32, 4000)\n",
        "        x = self.conv1(x)\n",
        "\n",
        "        # ---- Process first layer's output\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # ---- Layer 2\n",
        "        # conv2 takes (64, 32, 4000) and outputs (64, 2, 4000)\n",
        "        x = self.conv2(x)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcNVBOMb1hfo"
      },
      "source": [
        "# Loss weights can help prioritise properly predicting glycosites over the bulk unglycosylated sites\n",
        "# Priority here is predicting sites, not predicting non-sites, so we want a very high beta to distinguish between them still.\n",
        "\n",
        "def cb_weights(labels, beta = None):\n",
        "    \"\"\"Compute the weights for Class Balanced Loss.\n",
        "    Class Balanced Loss: ((1-beta)/(1-beta^n))*Loss(labels, logits)\n",
        "    where Loss is a loss function, here cross entropy loss.\n",
        "    Args:\n",
        "      labels: Pandas series of labels. Assumes binary 0/1 labeling.\n",
        "      beta: float. Hyperparameter for Class balanced loss or list of pre-set weights. If beta = None, returns standard (1, 30) weights.\n",
        "    Returns:\n",
        "      A set of weights to be supplied to the loss function.\n",
        "    \"\"\"\n",
        "    \n",
        "    if beta is None:\n",
        "      return (1, 30)\n",
        "    elif type(beta) == float: \n",
        "      total_samples = labels.apply(len).sum()\n",
        "      total_sites = labels.apply(sum).sum() # Assumes 0-1 labeling\n",
        "      samples_dist = [total_samples - total_sites, total_sites]\n",
        "\n",
        "      effective_num = 1.0 - np.power(beta, samples_dist)\n",
        "      weights = (1.0 - beta) / np.array(effective_num)\n",
        "      weights = weights / np.sum(weights) * len(samples_dist)\n",
        "      return weights\n",
        "    else:\n",
        "      try:\n",
        "        len(beta) == 2:\n",
        "        return beta\n",
        "      except:\n",
        "        print('Beta must be a float, a 2-length iterable, or None.')\n",
        "        raise ValueError"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anF-WMUyJ5Lj"
      },
      "source": [
        "**3. Define training and tuning functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UmeQMvnwixJ"
      },
      "source": [
        "def train(config, device, zip_path, info, epochs=10, tuning = False, checkpoint_dir = None):\n",
        "    \"\"\"Train the CNN model.\n",
        "    Args: \n",
        "      config: a dictionary with hyperparameter values {'kernel_size', 'beta', 'lr'}. \n",
        "        If tuning = True, supports ray.tune search spaces.\n",
        "      device: a pytorch device indicating whether the model should be loaded into cpu or gpu.\n",
        "      zip_path: a path to an zip/npz file containing the .npy arrays for each gene.\n",
        "      info: a pandas dataframe with gene names as keys, 'sequence' and 'label' keys as lists/iterables.\n",
        "      epochs: optional. an integer value, indicating the number of epochs (training loops) the training should last.\n",
        "      tuning: optional. a boolean indicating whether the model is ran in the context of ray.tune tuning.\n",
        "        In that case, it won't print training results, but will instead pass them to ray.tune.\n",
        "        Default = False.\n",
        "      checkpoint_dir: optional. only used when tuning = true. used to retrieve the best model's model_state after tuning.\n",
        "        Default = None.\n",
        "    Returns:\n",
        "      Doesn't return anything, but has modified the weights of the supplied model object. \n",
        "      \"\"\"\n",
        "\n",
        "    # Initialise model\n",
        "    model = Net(kernel_size = config['kernel_size'])\n",
        "    model.to(device)\n",
        "\n",
        "    # Load info and paths into dataset objects and create loaders\n",
        "    train_idx, val_idx = train_test_split(range(len(info)), test_size = 0.2)\n",
        "    loader_params = {'batch_size': 64, 'shuffle': True}\n",
        "\n",
        "    info_train = info.iloc[train_idx, :]\n",
        "    data_train = EmbeddingDataset(zip_path, info_train)\n",
        "    loader_train = DataLoader(data_train, **loader_params)\n",
        "\n",
        "    info_val = info.iloc[val_idx, :]\n",
        "    data_val = EmbeddingDataset(zip_path, info_val)\n",
        "    loader_val = DataLoader(data_val, **loader_params)\n",
        "\n",
        "    # Gather weights\n",
        "    weights = cb_weights(info_train['label'], beta = config['beta'])\n",
        "\n",
        "    # Initialise loss function and optimizer \n",
        "    loss_fn = nn.CrossEntropyLoss(weight = torch.FloatTensor(weights).to(device))\n",
        "    optimizer = optim.Adam(model.parameters(), lr = config['lr'], amsgrad = True) \n",
        "\n",
        "    if tuning and checkpoint_dir:\n",
        "        model_state, optimizer_state = torch.load(\n",
        "            os.path.join(checkpoint_dir, \"checkpoint\"))\n",
        "        model.load_state_dict(model_state)\n",
        "        optimizer.load_state_dict(optimizer_state)\n",
        "\n",
        "    # Start training loop\n",
        "    if not tuning:\n",
        "      print(\"Start training...\\n\")\n",
        "      print(f\"{'Epoch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Prec. @.5':^10} | {'Recall@.5':^10} | {'AUC':^10} | {'AP':^10}\")\n",
        "      print(\"-\"*87)\n",
        "\n",
        "    model = model.float()\n",
        "\n",
        "    for epoch_i in range(epochs):\n",
        "        # =======================================\n",
        "        #               Training\n",
        "        # =======================================\n",
        "\n",
        "        # Tracking time and loss\n",
        "        total_loss = 0\n",
        "\n",
        "        # Put the model into the training model\n",
        "        model.train()\n",
        "\n",
        "        for step, batch in enumerate(loader_train):\n",
        "            # Load batch to GPU\n",
        "            b_input_ids, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "            # Zero out any previously calculated gradients\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Perform a forward pass. (output shape: (batch, n_classes, length))\n",
        "            logits = model(b_input_ids.float())\n",
        "\n",
        "            # Compute loss and accumulate the loss values\n",
        "            loss = loss_fn(logits, b_labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Perform a backward pass to calculate gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Update parameters\n",
        "            optimizer.step()\n",
        "\n",
        "        # Calculate the average loss over the entire training data\n",
        "        avg_train_loss = total_loss / len(loader_train)\n",
        "\n",
        "        # =======================================\n",
        "        #               Evaluation\n",
        "        # =======================================\n",
        "        # After the completion of each training epoch, measure the model's\n",
        "        # performance on our validation set.\n",
        "        val_loss, val_accuracy, val_precision, val_recall, val_auc, val_ap = evaluate(model, loader_val, loss_fn)\n",
        "\n",
        "        # Print performance over the entire training data\n",
        "        if tuning:\n",
        "          with tune.checkpoint_dir(epoch_i) as checkpoint_dir:\n",
        "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
        "            torch.save((model.state_dict(), optimizer.state_dict()), path)\n",
        "          tune.report(loss = val_loss, precision = val_precision, recall = val_recall, auc = val_auc, ap = val_ap)\n",
        "        else:\n",
        "          print(f\"{epoch_i + 1:^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_precision:^12.2f} | {val_recall:^12.2f} | {val_auc:^10.4f} | {val_ap:^10.4f}\")\n",
        "            \n",
        "        \n",
        "        # # =======================================\n",
        "        # #               Checkpoint\n",
        "        # # =======================================\n",
        "\n",
        "        # torch.save(model.state_dict(), f\"model_{time}_{epoch_i}.pth\")\n",
        "\n",
        "        gc.collect()\n",
        "\n",
        "    print(f\"Training complete!\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLeCWHxmKFAb"
      },
      "source": [
        "def evaluate(model, val_dataloader, loss_fn = nn.CrossEntropyLoss):\n",
        "    \"\"\"Measure a model's performance on a validation set.\n",
        "    Args:\n",
        "      model: a model object to evaluate.\n",
        "      val_dataloader: a dataloader with validation data.\n",
        "      loss_fn: a loss function to calculate the validation loss with. \n",
        "        Usually passed on within train() to be the same loss function as used for training.\n",
        "        Default = nn.CrossEntropyLoss, but should be overwritten to match training loss_fn.\n",
        "    Returns:\n",
        "      val_loss: the mean of the loss across batches.\n",
        "      val_accuracy: the mean of the accuracy (correct predictions based on cutoff 0.5) across batches.\n",
        "      val_precision: the fraction of correct positive predictions based on cutoff 0.5 (also known as positive predictive value)\n",
        "      val_recall: the fraction of actual positives that were predicted by the model based on cutoff 0.5 (also known as sensitivity or true positive rate)\n",
        "      val_auc: the area under the ROC curve, indicates our model's capability to distinguish between the two classes.\n",
        "      val_ap: the average precision, aka the area under the precision-recall curve. indicates our model's capability to distinguish the positive values correctly.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled\n",
        "    # during the test time.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    val_loss = []\n",
        "    true_labs_all = []\n",
        "    probs_all = []\n",
        "    preds_all = []\n",
        "\n",
        "    # For each batch in our validation set...\n",
        "    for batch in val_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Compute scores (shape: (batch, n_classes, length))\n",
        "        with torch.no_grad():\n",
        "            scores = model(b_input.float())\n",
        "\n",
        "        # Compute loss\n",
        "        if loss_fn is not None:\n",
        "          loss = loss_fn(scores, b_labels)\n",
        "          val_loss.append(loss.item())\n",
        "\n",
        "        # Get the probabilities and predictions\n",
        "        true_labs = b_labels.cpu().numpy()\n",
        "        probs = F.softmax(scores, dim=1).cpu().numpy()[:, 1, :] # keep only probabilities for label 1\n",
        "        preds = torch.argmax(scores, dim=1).cpu().numpy()\n",
        "\n",
        "        # Save to compute AUC and average precision (from precision-recall curve) later\n",
        "        true_labs_all.append(true_labs.flatten())\n",
        "        probs_all.append(probs.flatten())\n",
        "        preds_all.append(preds.flatten())\n",
        "\n",
        "    # Compute the performance statistics over the entire test set\n",
        "    true_labs_all = np.hstack(true_labs_all)\n",
        "    probs_all = np.hstack(probs_all)\n",
        "    preds_all = np.hstack(preds_all)\n",
        "\n",
        "    if loss_fn is not None:\n",
        "      val_loss = np.mean(val_loss)\n",
        "    val_accuracy = (preds_all == true_labs_all).mean() * 100\n",
        "    val_precision = (preds_all[preds_all == 1] == true_labs_all[preds_all == 1]).mean()*100       # Also known as positive predictive value\n",
        "    val_recall = (preds_all[true_labs_all == 1] == true_labs_all[true_labs_all == 1]).mean()*100  # Also known as sensitivity or true positive rate\n",
        "    val_auc = roc_auc_score(y_true = true_labs_all, y_score = probs_all)                          # Area under ROC curve\n",
        "    val_ap = average_precision_score(y_true = true_labs_all, y_score = probs_all)                 # Area under position-recall curve\n",
        "\n",
        "    return val_loss, val_accuracy, val_precision, val_recall, val_auc, val_ap"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xi4WU27lkZXG"
      },
      "source": [
        "def tune_model(config, device, num_samples):\n",
        "  \"\"\"A function to tune models to find the best hyperparameters using ray.tune.\n",
        "  Args:\n",
        "    config: a configuration dictionary with tune search space indicators.\n",
        "    device: a pytorch device indicating whether the model should be loaded into cpu or gpu.\n",
        "  Returns:\n",
        "    A tune.ExperimentAnalysis object with information about the best trial. \n",
        "    Can be used in build_best_model() to reconstitute the model.\"\"\"\n",
        "\n",
        "  # Start training/tuning\n",
        "  scheduler = ASHAScheduler(\n",
        "      metric = \"ap\", # alternative: loss, min\n",
        "      mode = \"max\",\n",
        "      max_t = 20,\n",
        "      grace_period = 3,\n",
        "      reduction_factor = 2)\n",
        "  reporter = JupyterNotebookReporter(\n",
        "      overwrite = True,\n",
        "      metric_columns = [\"loss\", \"auc\", \"ap\", \"precision\", \"recall\", \"training_iteration\"])\n",
        "  search_alg = HyperOptSearch(\n",
        "      metric = \"ap\",\n",
        "      mode = \"max\")\n",
        "\n",
        "  result = tune.run(\n",
        "      partial(train, device = device, zip_path = zip_path, info = info_trainval, epochs = 20, tuning = True),\n",
        "      resources_per_trial = {\"gpu\": 1},\n",
        "      config = config,\n",
        "      num_samples = num_samples,\n",
        "      search_alg = search_alg,\n",
        "      scheduler = scheduler,\n",
        "      progress_reporter = reporter)\n",
        "\n",
        "  return result\n",
        "\n",
        "\n",
        "def build_best_model(best_trial, device):\n",
        "  best_trained_model = Net(kernel_size = best_trial.config[\"kernel_size\"])\n",
        "  best_trained_model.to(device)\n",
        "\n",
        "  best_checkpoint_dir = best_trial.checkpoint.value\n",
        "  model_state, optimizer_state = torch.load(os.path.join(best_checkpoint_dir, \"checkpoint\"))\n",
        "  best_trained_model.load_state_dict(model_state)\n",
        "  return best_trained_model"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqfxuEaUNS9j"
      },
      "source": [
        "<b>3 - option a. Tune the model</b>\n",
        "\n",
        "This option will train the model `num_samples` times, trying different combinations of hyperparameters each time, and return the best one.  \n",
        "\n",
        "`num_samples` has different interactions with random parameter selections (like `tune.choice()` or `tune.loguniform()`) and grid search. One unit of `num_samples` leads to only one sample from all random parameters, but one full grid search of all parameters (i.e. `num_workers` = 1 with `tune.grid_search(['A', 'B', 'C'])` makes three trials!). For an explanation on how exactly `num_samples` works, see [here](https://docs.ray.io/en/latest/tune/api_docs/search_space.html).  \n",
        "\n",
        "\n",
        "For a non-tuning option, see 3b."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2iVXxsVdO0u",
        "outputId": "440705dc-e94f-434e-cb15-1d1ee1063ab5"
      },
      "source": [
        "# cb_weights(info['label'], 0.99999)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.03512864, 1.96487136])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "id": "Agz1MjLfvpG5",
        "outputId": "d22810a1-741b-42cd-9c89-b8e100ad2056"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Select hyperparameters\n",
        "config = {\n",
        "    \"kernel_size\": tune.choice([5, 7, 9, 13]),\n",
        "    # \"kernel_size\": tune.qrandint(5, 13, 2),\n",
        "    # \"beta\": tune.choice([0.9999, 0.99999, (1, 30)]),\n",
        "    \"beta\": tune.loguniform(0.9999, 0.99999),\n",
        "    \"lr\": 0.001}\n",
        "\n",
        "# Tune model\n",
        "best_trial = tune_model(config, device, num_samples = 10) # Num_samples to # desired runs (like 10) if random choices, to 1 if grid search (will run entire grid once)\n",
        "\n",
        "# Reconstitute best model\n",
        "model = build_best_model(best_trial, device)\n",
        "\n",
        "# Show tensorboard output\n",
        "# !tensorboard --logdir=~/ray_results/my_experiment"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 3.1/12.7 GiB<br>Using AsyncHyperBand: num_stopped=10\n",
              "Bracket: Iter 12.000: 0.10036793275573325 | Iter 6.000: 0.08741672207589907 | Iter 3.000: 0.03821025901460351<br>Resources requested: 0/2 CPUs, 0/1 GPUs, 0.0/7.33 GiB heap, 0.0/3.66 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /root/ray_results/DEFAULT_2021-09-11_15-30-52<br>Number of trials: 10/10 (10 TERMINATED)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name      </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">    beta</th><th style=\"text-align: right;\">  kernel_size</th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">     loss</th><th style=\"text-align: right;\">     auc</th><th style=\"text-align: right;\">         ap</th><th style=\"text-align: right;\">  precision</th><th style=\"text-align: right;\">  recall</th><th style=\"text-align: right;\">  training_iteration</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>DEFAULT_3f9cc9ee</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.999937</td><td style=\"text-align: right;\">            5</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.0339966</td><td style=\"text-align: right;\">0.926077</td><td style=\"text-align: right;\">0.133444   </td><td style=\"text-align: right;\">  nan      </td><td style=\"text-align: right;\">  0     </td><td style=\"text-align: right;\">                  20</td></tr>\n",
              "<tr><td>DEFAULT_40536f1e</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.999943</td><td style=\"text-align: right;\">            5</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.0655836</td><td style=\"text-align: right;\">0.903175</td><td style=\"text-align: right;\">0.0705934  </td><td style=\"text-align: right;\">  nan      </td><td style=\"text-align: right;\">  0     </td><td style=\"text-align: right;\">                  12</td></tr>\n",
              "<tr><td>DEFAULT_1e759c2e</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.999955</td><td style=\"text-align: right;\">            5</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.0947441</td><td style=\"text-align: right;\">0.921972</td><td style=\"text-align: right;\">0.0669419  </td><td style=\"text-align: right;\">  nan      </td><td style=\"text-align: right;\">  0     </td><td style=\"text-align: right;\">                  12</td></tr>\n",
              "<tr><td>DEFAULT_1cfe895a</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.999968</td><td style=\"text-align: right;\">           13</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.023049 </td><td style=\"text-align: right;\">0.970192</td><td style=\"text-align: right;\">0.204706   </td><td style=\"text-align: right;\">   18.1924 </td><td style=\"text-align: right;\"> 55.0877</td><td style=\"text-align: right;\">                  20</td></tr>\n",
              "<tr><td>DEFAULT_6542918a</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.999959</td><td style=\"text-align: right;\">            9</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.0540958</td><td style=\"text-align: right;\">0.927753</td><td style=\"text-align: right;\">0.0818549  </td><td style=\"text-align: right;\">  nan      </td><td style=\"text-align: right;\">  0     </td><td style=\"text-align: right;\">                  12</td></tr>\n",
              "<tr><td>DEFAULT_0b863e38</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.999988</td><td style=\"text-align: right;\">           13</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.0825802</td><td style=\"text-align: right;\">0.94304 </td><td style=\"text-align: right;\">0.191116   </td><td style=\"text-align: right;\">   14.5583 </td><td style=\"text-align: right;\"> 64.375 </td><td style=\"text-align: right;\">                  20</td></tr>\n",
              "<tr><td>DEFAULT_a407e676</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.999934</td><td style=\"text-align: right;\">           13</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.492432 </td><td style=\"text-align: right;\">0.159278</td><td style=\"text-align: right;\">0.000369411</td><td style=\"text-align: right;\">  nan      </td><td style=\"text-align: right;\">  0     </td><td style=\"text-align: right;\">                   3</td></tr>\n",
              "<tr><td>DEFAULT_51bea394</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.99991 </td><td style=\"text-align: right;\">            5</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.528582 </td><td style=\"text-align: right;\">0.150778</td><td style=\"text-align: right;\">0.000337385</td><td style=\"text-align: right;\">  nan      </td><td style=\"text-align: right;\">  0     </td><td style=\"text-align: right;\">                   3</td></tr>\n",
              "<tr><td>DEFAULT_5bec40f0</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.999954</td><td style=\"text-align: right;\">           13</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.0194309</td><td style=\"text-align: right;\">0.942688</td><td style=\"text-align: right;\">0.19278    </td><td style=\"text-align: right;\">   16.9394 </td><td style=\"text-align: right;\"> 57.8947</td><td style=\"text-align: right;\">                  20</td></tr>\n",
              "<tr><td>DEFAULT_63d39f38</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.999987</td><td style=\"text-align: right;\">           13</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.238874 </td><td style=\"text-align: right;\">0.826304</td><td style=\"text-align: right;\">0.0543146  </td><td style=\"text-align: right;\">    5.71319</td><td style=\"text-align: right;\"> 64.2241</td><td style=\"text-align: right;\">                   6</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-09-11 21:05:22,064\tINFO tune.py:561 -- Total run time: 20070.12 seconds (20069.31 seconds for the tuning loop).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best trial config: {'kernel_size': 13, 'beta': 0.9999538920913715, 'lr': 0.001}\n",
            "Best trial final validation loss: 0.019430930105348427\n",
            "Best trial final validation average precision: 0.19278001602192688\n",
            "Best trial final validation auc: 0.9426880381350121\n",
            "2021-09-11 21:05:43.289042: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-11 21:05:43.893115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-11 21:05:43.894052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "\n",
            "NOTE: Using experimental fast data loading logic. To disable, pass\n",
            "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
            "    https://github.com/tensorflow/tensorboard/issues/4784\n",
            "\n",
            "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
            "TensorBoard 2.6.0 at http://localhost:6006/ (Press CTRL+C to quit)\n",
            "^C\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-09-11 21:08:53,423\tWARNING tune.py:519 -- SIGINT received (e.g. via Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C one more time (or send SIGINT/SIGKILL/SIGTERM) to skip. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nppYbtIiO-3Z"
      },
      "source": [
        "**3 - option b: Train without tuning**\n",
        "\n",
        "Run a simple 20-epoch training sequence with set parameters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "kjenIAgG3AIR",
        "outputId": "331ea2e2-35d8-4a4b-e70e-178d065f2849"
      },
      "source": [
        "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# config = {\n",
        "#     \"kernel_size\": 7,\n",
        "#     \"beta\": (1, 30), # Pass a 2-length list as beta to pre-set weights, pass a 1-length float between 0.99 and 0.99999 to get class-balanced weights\n",
        "#     \"lr\": 0.001}\n",
        "# train(config, device, zip_path, info_trainval, epochs = 20, tuning = False)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-dfbf228326b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"beta\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \"lr\": 0.001}\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo_trainval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuning\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-99d00c42b962>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(config, device, zip_path, info, epochs, tuning, checkpoint_dir)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Initialise model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'kernel_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Gather weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 852\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m     def register_backward_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    550\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    848\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    849\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 850\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVtP06ulzB8D"
      },
      "source": [
        "**4. Check performance on the test set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QT1bef-Hs_LG",
        "outputId": "ebad2b31-9b8a-49c7-fe4a-4e495d292af8"
      },
      "source": [
        "# print(best_trial.)\n",
        "\n",
        "# # Show best trial\n",
        "# best_trial = result.get_best_trial(\"loss\", \"min\", \"last-5-avg\")\n",
        "print(f\"Best trial config: {best_trial.config}\")\n",
        "print(f\"Best trial final validation loss: {best_trial.last_result['loss']}\")\n",
        "print(f\"Best trial final validation average precision: {best_trial.last_result['ap']}\")\n",
        "print(f\"Best trial final validation auc: {best_trial.last_result['auc']}\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best trial config: {'kernel_size': 13, 'beta': 0.9999538920913715, 'lr': 0.001}\n",
            "Best trial final validation loss: 0.019430930105348427\n",
            "Best trial final validation average precision: 0.19278001602192688\n",
            "Best trial final validation auc: 0.9426880381350121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdHhnVyJKau7",
        "outputId": "11645b8f-6108-473e-a884-096a610168b5"
      },
      "source": [
        "# Build the test set\n",
        "data_test = EmbeddingDataset(zip_path, info_test)\n",
        "loader_test = DataLoader(data_test, **{'batch_size': 16, 'shuffle': True})\n",
        "\n",
        "# Run the best model on the test set\n",
        "test_loss, test_accuracy, test_precision, test_recall, test_auc, test_ap = evaluate(model, loader_test, loss_fn=None)\n",
        "print(f\"Test AUC: {test_auc:.4f} | Test AP:  {test_ap:.4f} | Test precision (cutoff=0.5): {test_precision:.2f} | Test recall (cutoff=0.5): {test_recall:.2f}\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test AUC: 0.9754 | Test AP:  0.2151 \n",
            " Test precision (cutoff=0.5): 17.53 | Test recall (cutoff=0.5): 57.56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YojIJ3DUMlur"
      },
      "source": [
        "<b>5. Save and export the model</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoR_FZVFEocN"
      },
      "source": [
        "torch.save(model.state_dict(), \"model_params.pth\")\n",
        "# torch.save(model, \"model_full.pth\")"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qvl7j9prEpi3"
      },
      "source": [
        "!cp model_params.pth /content/drive/MyDrive/NetOGlyc/"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyuaNZBsiQiV"
      },
      "source": [
        "drive.flush_and_unmount()"
      ],
      "execution_count": 23,
      "outputs": []
    }
  ]
}